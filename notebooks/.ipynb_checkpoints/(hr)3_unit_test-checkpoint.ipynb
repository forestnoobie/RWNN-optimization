{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hr Neural Network Train Test\n",
    "* stage block 구성 완료\n",
    "* 학습 가능한지 점검"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* toolbox 구성\n",
    "* 임의로 graphs 형성 (불러오기 하지말고 바로 넘겨 받을 수 있게 설정)\n",
    "* 학습 및 acc 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import base, creator\n",
    "from deap import tools\n",
    "\n",
    "\n",
    "import random\n",
    "from itertools import repeat\n",
    "from collections import Sequence\n",
    "\n",
    "# For evaluate function --------------------------\n",
    "import glob\n",
    "from easydict import EasyDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn    # for hardware tunning (cudnn.benchmark = True)\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "import logging\n",
    "\n",
    "# Gray code package\n",
    "from utils_kyy.utils_graycode_v2 import *\n",
    "\n",
    "# custom package in utils_kyy\n",
    "from utils_kyy.utils_graph import load_graph\n",
    "from utils_kyy.models_hr import RWNN\n",
    "from utils_kyy.train_validate import train, validate, train_AMP\n",
    "from utils_kyy.lr_scheduler import LRScheduler\n",
    "from torchsummary import summary\n",
    "# -------------------------------------------------\n",
    "\n",
    "#from apex import amp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage_pool_path_list ~ graphs 만들어주는 과정 -> indivudal에서 바로 생성하는 걸로!\n",
    "\n",
    "\n",
    "def evaluate_hr_full_train(individual, args_train, stage_pool_path_list, data_path=None, channels=109, log_file_name=None):  # individual\n",
    "    \n",
    "#     # 1) load graph\n",
    "#     total_graph_path_list = []\n",
    "#     for i in range(3):\n",
    "#         temp = glob.glob(stage_pool_path_list[i] + '*.yaml') # sorting 해줘야함\n",
    "#         temp.sort()\n",
    "#         total_graph_path_list.append( temp )        \n",
    "# #         total_graph_path_list.append( glob.glob(stage_pool_path_list[i] + '*.yaml') )\n",
    "    \n",
    "    # grape \n",
    "\n",
    "    graph_name = []\n",
    "\n",
    "    # args_train 셋팅에서 graycode 변환이 true 인지 확인\n",
    "    if args_train.graycode:\n",
    "        ## Decode 해줘야 !\n",
    "        gray_len = len(individual)//3\n",
    "        for i in range(3):\n",
    "            # list to string\n",
    "            tmp = ''\n",
    "            for j in individual[gray_len*i:gray_len*(i+1)]:\n",
    "                tmp += str(j)\n",
    "\n",
    "            # sting to binary to num\n",
    "            graph_name.append(graydecode(int(tmp)))\n",
    "\n",
    "    else :\n",
    "        graph_name = individual\n",
    "\n",
    "    stage_1_graph = load_graph( total_graph_path_list[0][graph_name[0]] )\n",
    "    stage_2_graph = load_graph( total_graph_path_list[1][graph_name[1]] )\n",
    "    stage_3_graph = load_graph( total_graph_path_list[2][graph_name[2]] )\n",
    "    \n",
    "    graphs = EasyDict({'stage_1': stage_1_graph,\n",
    "                       'stage_2': stage_2_graph,\n",
    "                       'stage_3': stage_3_graph\n",
    "                      })\n",
    "\n",
    "    # 2) build RWNN\n",
    "    channels = channels\n",
    "    NN_model = RWNN(net_type='small', graphs=graphs, channels=channels, num_classes=args_train.num_classes, input_channel=args_train.input_dim)\n",
    "    NN_model.cuda()\n",
    "\n",
    "    ###########################\n",
    "    # Flops 계산 - [Debug] nn.DataParallele (for multi-gpu) 적용 전에 확인.\n",
    "    ###########################\n",
    "    input_flops = torch.randn(1, args_train.input_dim, 32, 32).cuda()\n",
    "    flops, params = profile(NN_model, inputs=(input_flops, ), verbose=False)\n",
    "\n",
    "    ## Model summary\n",
    "    #summary(NN_model, input_size=(1, 224, 224))\n",
    "\n",
    "    # 3) Prepare for train### 일단 꺼보자!\n",
    "    #NN_model = nn.DataParallel(NN_model)  # for multi-GPU\n",
    "    NN_model = nn.DataParallel(NN_model, device_ids=[0,1,2,3])\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD(NN_model.parameters(), args_train.base_lr,\n",
    "                                momentum=args_train.momentum,\n",
    "                                weight_decay=args_train.weight_decay)\n",
    "    \n",
    "    start_epoch  = 0\n",
    "    best_prec1 = 0    \n",
    "    \n",
    "    cudnn.benchmark = True    # This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware.  \n",
    "    \n",
    "    ###########################\n",
    "    # Dataset & Dataloader\n",
    "    ###########################\n",
    "\n",
    "    # 이미 다운 받아놨으니 download=False\n",
    "    # 데이터가 없을 경우, 처음에는 download=True 로 설정해놓고 실행해주어야함\n",
    "    \n",
    "    if data_path is None :\n",
    "        data_path = './data'\n",
    "    \n",
    " \n",
    "    if args_train.data == \"CIFAR10\" :\n",
    "\n",
    "        cutout_length = 16  # from nsga-net github\n",
    "        \n",
    "        CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]  # from nsga-net github\n",
    "        CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "        \n",
    "        train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                Cutout(cutout_length),  # from nsga-net github\n",
    "                transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "            ])\n",
    "\n",
    "        val_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "            ])\n",
    "\n",
    "        train_dataset = torchvision.datasets.CIFAR10(root=data_path, train=True,\n",
    "                                                download=True, transform=train_transform)\n",
    "\n",
    "        val_dataset = torchvision.datasets.CIFAR10(root=data_path, train=False,\n",
    "                                               download=True, transform=val_transform)\n",
    "        \n",
    "    else :\n",
    "        raise Exception(\"Data Error, Only CIFAR10 allowed for the moment\")\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args_train.batch_size,\n",
    "                                              shuffle=True, num_workers=args_train.workers)  \n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args_train.batch_size,\n",
    "                                             shuffle=False, num_workers=args_train.workers)    \n",
    "    \n",
    "    ###########################\n",
    "    # Train\n",
    "    ###########################\n",
    "    niters = len(train_loader)\n",
    "    niters = 1\n",
    "\n",
    "    lr_scheduler = LRScheduler(optimizer, niters, args_train)  # (default) args.step = [30, 60, 90], args.decay_factor = 0.1, args.power = 2.0\n",
    "    epoch_ = 0\n",
    "    \n",
    "    for epoch in range(start_epoch, args_train.epochs):\n",
    "        # train for one epoch\n",
    "        train(train_loader, NN_model, criterion, optimizer, lr_scheduler, epoch, args_train.print_freq, log_file_name)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, NN_model, criterion, epoch, log_file_name)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "#         is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        \n",
    "        epoch_ = epoch\n",
    "\n",
    "    return (-best_prec1, flops), epoch_  # Min (-val_accuracy, flops) 이므로 val_accuracy(top1)에 - 붙여서 return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_kyy.utils_hr import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 예시 생성\n",
    "grph_ex =make_random_graph_ex(15)\n",
    "\n",
    "## G matrix 구하기\n",
    "random.seed(1234)\n",
    "nds, inds,onds = get_graph_info(grph_ex)\n",
    "g_mat = get_g_matrix(nds)\n",
    "\n",
    "from utils_kyy.utils_hr import operation_dictionary\n",
    "operation_dict=operation_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encoding g_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## g_mat -> 한 차원으로 피는 것 (encoding individual 만드는 것)\n",
    "\n",
    "nsize =  g_mat.shape[0]\n",
    "g_encode = []\n",
    "\n",
    "for idx, col_idx in enumerate(range(1,nsize)):\n",
    "    row_idx = idx + 1\n",
    "    g_encode.extend(g_mat[:row_idx,col_idx].tolist())\n",
    "    \n",
    "## encoding 된 걸 다시 g_mat 복원\n",
    "\n",
    "decode_mat = np.ones((nsize,nsize)) \n",
    "# to initlize 7 (disconnected)\n",
    "decode_mat = decode_mat * 7\n",
    "\n",
    "\n",
    "row_idx = 0\n",
    "col_idx = 1\n",
    "for idx , v in enumerate(g_encode):\n",
    "    decode_mat[row_idx][col_idx] = v\n",
    "    \n",
    "    if row_idx + 1  == col_idx :\n",
    "        col_idx+= 1\n",
    "        row_idx = 0 \n",
    "    else :\n",
    "        row_idx += 1\n",
    "        \n",
    "        \n",
    "def gmat2ind(g_mat):\n",
    "    nsize =  g_mat.shape[0]\n",
    "    g_encode = []\n",
    "\n",
    "    for idx, col_idx in enumerate(range(1,nsize)):\n",
    "        row_idx = idx + 1\n",
    "        g_encode.extend(g_mat[:row_idx,col_idx].tolist())\n",
    "\n",
    "    return g_encode\n",
    "\n",
    "def ind2gmat(g_encode,nize):\n",
    "    decode_mat = np.ones((nsize,nsize)) \n",
    "    \n",
    "    # to initlize 7 (disconnected)\n",
    "    decode_mat = decode_mat * 7\n",
    "\n",
    "\n",
    "    row_idx = 0\n",
    "    col_idx = 1\n",
    "    for idx , v in enumerate(g_encode):\n",
    "        decode_mat[row_idx][col_idx] = v\n",
    "\n",
    "        if row_idx + 1  == col_idx :\n",
    "            col_idx+= 1\n",
    "            row_idx = 0 \n",
    "        else :\n",
    "            row_idx += 1\n",
    "            \n",
    "    return decode_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters \n",
    "args_train = EasyDict({\n",
    "    \n",
    "    'data_path' : 'D:/data/',\n",
    "    \n",
    "        \"data\": \"CIFAR10\",\n",
    "        \"num_classes\" : 10,\n",
    "        \n",
    "        \"graycode\" : True,\n",
    "        \n",
    "        \"input_dim\" : 3,\n",
    "        \"lr_mode\": \"cosine\",\n",
    "        \"warmup_mode\": \"linear\",    \n",
    "        \"base_lr\": 0.2,\n",
    "        \"momentum\": 0.9, \n",
    "        \"weight_decay\": 0.00005,\n",
    "        \"print_freq\": 100,\n",
    "        \n",
    "        \"epochs\": 2,\n",
    "        \"batch_size\": 80,\n",
    "        \"workers\": 0,\n",
    "        \n",
    "        \"warmup_epochs\": 0,\n",
    "        \"warmup_lr\": 0.0,\n",
    "        \"targetlr\": 0.0,\n",
    "    \n",
    "        \"nsize\" : 25\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "})\n",
    "\n",
    "data_path = args_train.data_path\n",
    "channels=109\n",
    "num_classes=10\n",
    "input_channel=3\n",
    "\n",
    "log_file_name = 'temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ind -> graph\n",
    "\n",
    "individual = gmat2ind(g_mat)\n",
    "\n",
    "graphs_list = [gmat2graph(g_mat) for i in range(3)]\n",
    "\n",
    "graphs = EasyDict({'stage_1': graphs_list[0],\n",
    "                   'stage_2': graphs_list[1],\n",
    "                   'stage_3': graphs_list[2]\n",
    "                  })\n",
    "\n",
    "\n",
    "\n",
    "# ind -> gmat\n",
    "\n",
    "gmats = [ind2gmat(individual,nsize) for i in range(3)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_kyy.utils_hr import operation_dictionary\n",
    "operation_dict=operation_dictionary()\n",
    "\n",
    "class double_unit(nn.Module):\n",
    "    def __init__(self, inplanes, outplanes, stride=1):\n",
    "        super(double_unit, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm2d(outplanes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(x)\n",
    "        #print(\"Double RELU output\",out.size())\n",
    "        out = self.bn(out)\n",
    "        #print(\"Double BN output\",out.size())\n",
    "        return out\n",
    "        \n",
    "class Triplet_unit(nn.Module):\n",
    "    def __init__(self, inplanes, outplanes, stride=1):\n",
    "        super(Triplet_unit, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv = depthwise_separable_conv_3x3(inplanes, outplanes, stride)\n",
    "        self.bn = nn.BatchNorm2d(outplanes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(x)\n",
    "        #print(\"Triplet Relu output\",out.size())\n",
    "        out = self.conv(out)\n",
    "        #print(\"Triplet Conv output\",out.size())\n",
    "        out = self.bn(out)\n",
    "        #print(\"Triplet BN output\",out.size())\n",
    "        return out\n",
    "\n",
    "class Node_OP(nn.Module):\n",
    "    def __init__(self, Node, inplanes, outplanes, gmat):\n",
    "        super(Node_OP, self).__init__()\n",
    "        self.is_input_node = Node.type == 0\n",
    "        self.input_nums = len(Node.inputs)  # 해당 Node에 input으로 연결된 노드의 개수\n",
    "        \n",
    "        ## For hierachical representation\n",
    "        self.id = Node.id\n",
    "        self.oplist = nn.ModuleList()\n",
    "        self.gmat = gmat\n",
    "        self.Node = Node\n",
    "        \n",
    "        #print(\"is it input {}, how many inputs {}\".format(self.is_input_node,self.input_nums))\n",
    "        # get operation from g_matirx\n",
    "        if self.input_nums >= 1:\n",
    "            for in_idx in Node.inputs:\n",
    "                operation_idx = gmat[in_idx][self.id]\n",
    "                op = operation_dict[operation_idx]\n",
    "                self.oplist.append(op(nin=outplanes, nout=outplanes, stride=1)) ## input node가 아닐 떄는 channel size 유지!!\n",
    "\n",
    "        # input 개수가 1보다 크면, 여러 input을 합쳐야함.\n",
    "        #print(\"input nums\",self.input_nums)\n",
    "        if self.input_nums >= 1:\n",
    "            self.mean_weight = nn.Parameter(torch.ones(self.input_nums))  # type: torch.nn.parameter.Parameter\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        if self.is_input_node:\n",
    "            self.conv = Triplet_unit(inplanes, outplanes, stride=2) \n",
    "        else:\n",
    "            self.conv = double_unit(outplanes, outplanes, stride=1)\n",
    "\n",
    "        \n",
    "    # [참고] nn.Sigmoid()(torch.ones(1)) = 0.7311\n",
    "    # seoungwonpark source 에서는 torch.zeros()로 들어감. => 0.5\n",
    "    def forward(self, *input):\n",
    "        #print(\"Node ID : {} input nums forward {} is it input node {}\".format(self.id,self.input_nums,self.is_input_node))\n",
    "        #print(\"Operation list\", self.oplist)\n",
    "        if self.input_nums > 1 and self.is_input_node == False:\n",
    "            #print('='*10)\n",
    "            #print(\"Node ID {} Components mean_weight : {} Operation type : {} input size : {}\".format(self.id, self.mean_weight[0].size(),\n",
    "                                                                                                    # self.oplist[0],input[0].size()))\n",
    "            out = self.sigmoid(self.mean_weight[0]) * self.oplist[0](input[0])  ## input node가 아니라고 생각해도 됨!\n",
    "            #print(\"Node ID : {} Output : {}\".format(self.id,out.size()))\n",
    "            for i in range(1, self.input_nums):\n",
    "                #print(\"input index {}, input : {} Operation Type {}\".format(i, input[i].size(),self.oplist[i]))\n",
    "                out = out + self.sigmoid(self.mean_weight[i]) * self.oplist[i](input[i])\n",
    "                #print(\"Input Node ID : {} Output : {}\".format(id,out.size()))\n",
    "\n",
    "            #print(\"Multi input Node output\",out)\n",
    "        elif self.input_nums == 1  and self.is_input_node == False:\n",
    "            out = self.sigmoid(self.mean_weight[0]) * self.oplist[0](input[0])  ## input node가 아니라고 생각해도 됨!\n",
    "            #print(\"Components \\n self.mean_weight : {} Operation : {} Sigmoid : {} input : {} \".format(self.mean_weight[0], self.oplist[0],self.sigmoid(self.mean_weight[0]),input[0].size()))\n",
    "\n",
    "            #print(\"node id : {} \\n after sigmoid Output : {}\".format(self.id,out.size()))\n",
    "\n",
    "        else :\n",
    "            out = input[0]\n",
    "            #print(\"Node ID : {} is a input Node \\n Output : {}\".format(self.id,out))\n",
    "\n",
    "        #print(\"node id : {}, before conv Out put : {}\".format(self.id,out))\n",
    "        out = self.conv(out)\n",
    "        \n",
    "        #print('='*50)\n",
    "        #print(\"node id : {}, Final Out put : {}\".format(self.id,out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class StageBlock(nn.Module):\n",
    "    def __init__(self, graph, inplanes, outplanes, gmat):\n",
    "        super(StageBlock, self).__init__()\n",
    "        # graph를 input으로 받아서, Node_OP class. 즉, neural network graph로 전환함.\n",
    "        self.nodes, self.input_nodes, self.output_nodes = get_graph_info(graph)\n",
    "        self.nodeop = nn.ModuleList()  # Holds submodules in a list.\n",
    "        self.gmat = gmat\n",
    "\n",
    "        for node in self.nodes:\n",
    "            # 각각의 node들을 Node_OP class로 만들어준 뒤, nn.ModuleList()인 self.nodeop에 append 해주기\n",
    "            self.nodeop.append(Node_OP(node, inplanes, outplanes, self.gmat))\n",
    "\n",
    "    def forward(self, x):\n",
    "        results = {}\n",
    "        # input\n",
    "        for id in self.input_nodes:\n",
    "            results[id] = self.nodeop[id](x)  # input x를 먼저 graph's input node에 각각 넣어줌.\n",
    "        # graph 중간 계산\n",
    "        for id, node in enumerate(self.nodes):\n",
    "            # 각각의 노드 id에 대해\n",
    "            if id not in self.input_nodes:\n",
    "                # graph's input node가 아니라면, 그래프 내에서 해당 노드의 인풋들인 node.inputs의 output인 results[_id]\n",
    "                #    => 그 결과를 results[id]에 저장.\n",
    "                # self.nodeop[id]는 해당 id의 Node_OP. 즉, input들을 받아서 forward(모아서, conv 태우기)하는 것.\n",
    "                # 따라서, input으로 넣을 때 unpack 함.\n",
    "                # id 작은 노드부터 result를 차근차근 계산하면서, id를 올라감.\n",
    "                #print(\"input size if id {}\".format(id))\n",
    "                results[id] = self.nodeop[id](*[results[_id] for _id in node.inputs])\n",
    "                \n",
    "                ## inspecting None\n",
    "                #print(self.nodeop[id](*[results[_id] for _id in node.inputs]))\n",
    "                #print(\"*** Starting id out : {}, ids in : {} , results : {}  node operation : {}\".format(id,node.inputs,results,self.nodeop[id]))\n",
    "                #print('='*30)\n",
    "                #print(\"Is Input node :  {} , Result id : {} Result Output shape : {} nodeop : {} \".format(id in self.input_nodes,\n",
    "                                                                                     #   id,results[id], self.nodeop[id]))\n",
    "\n",
    "        result = results[self.output_nodes[0]]\n",
    "        # output\n",
    "        # graph's output_nodes의 output 들을 평균내기\n",
    "        for idx, id in enumerate(self.output_nodes):\n",
    "            if idx > 0:\n",
    "                result = result + results[id]\n",
    "        result = result / len(self.output_nodes)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "class RWNN(nn.Module):\n",
    "    def __init__(self, net_type , graphs, gmats, channels, num_classes=1000, input_channel=3):\n",
    "        super(RWNN, self).__init__()\n",
    "\n",
    "        self.input_channel = input_channel\n",
    "        self.gmats = gmats\n",
    "        # 논문에서도 conv1 쪽은 예외적으로 Conv-BN 이라고 언급함. (나머지에서는 Conv-ReLU-BN 을 conv 로 표기)\n",
    "        #         self.conv1 = depthwise_separable_conv_3x3(input_channel, channels // 2, 2)    # nin, nout, stride\n",
    "        self.conv1 = depthwise_separable_conv_3x3(input_channel, channels // 2, 1)  # nin, nout, stride\n",
    "        self.bn1 = nn.BatchNorm2d(channels // 2)\n",
    "\n",
    "        # 채널수 변화도, 논문에서처럼 conv2: C, conv3: 2C, conv4: 4C, conv5: 8C\n",
    "        if net_type == 'small':\n",
    "            #             self.conv2 = Triplet_unit(channels // 2, channels, 2)    # inplanes, outplanes, stride=2\n",
    "            self.conv2 = Triplet_unit(channels // 2, channels, 1)  # inplanes, outplanes, stride=1\n",
    "\n",
    "            #             self.conv3 = StageBlock(graphs.stage_1, channels // 2, channels)\n",
    "            self.conv3 = StageBlock(graphs.stage_1, channels, channels,self.gmats[0])\n",
    "\n",
    "            self.conv4 = StageBlock(graphs.stage_2, channels, channels * 2, self.gmats[1])\n",
    "\n",
    "            self.conv5 = StageBlock(graphs.stage_3, channels * 2, channels * 4, self.gmats[2])\n",
    "\n",
    "            self.relu = nn.ReLU()\n",
    "\n",
    "            self.bn2 = nn.BatchNorm2d(channels * 4)\n",
    "            self.avgpool = nn.AvgPool2d(4, stride=1)  # 마지막은 global average pooling\n",
    "            self.fc = nn.Linear(channels * 4, num_classes)\n",
    "\n",
    "        #         if net_type == 'small':\n",
    "        #             self.conv2 = Triplet_unit(channels // 2, channels, 2)    # inplanes, outplanes, stride=2\n",
    "\n",
    "        #             self.conv3 = StageBlock(graphs.stage_1, channels, channels)\n",
    "\n",
    "        #             self.conv4 = StageBlock(graphs.stage_2, channels, channels *2)\n",
    "\n",
    "        #             self.conv5 = StageBlock(graphs.stage_3, channels * 2, channels * 4)\n",
    "\n",
    "        #             self.relu = nn.ReLU()\n",
    "        # #             self.conv = nn.Conv2d(channels * 4, 1280, kernel_size=1)   # 마지막에 1x1 conv, 1280-d\n",
    "        # #             self.bn2 = nn.BatchNorm2d(1280)\n",
    "\n",
    "        #######################################\n",
    "        # 원 코드에서 regular 부분 지움\n",
    "        #######################################\n",
    "        #         self.avgpool = nn.AvgPool2d(7, stride=1)  # 마지막은 global average pooling\n",
    "        #         self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"==========Data Size\", x.size())\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        #print(\"==========Conv1 Size\", x.size())  # => (-, 54, 16, 16)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        #print(\"==========Conv2  Size\", x.size())  # => (-, 109, 8, 8)\n",
    "        x = self.conv3(x)\n",
    "        #print(\"==========Conv3  Size\", x.size())  # => (-, 109, 8, 8)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        #         print(\"==========Conv4  Size\", x.size())  # => (-, 218, 4, 4)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        #print(\"==========Conv5  Size\", x.size())  # => (-, 436, 2, 2)\n",
    "\n",
    "        #         x = self.relu(x)  # => (-, 436, 1, 1)\n",
    "        #         x = self.conv(x)\n",
    "        #         print(\"==========Conv6  Size\", x.size())  # => (-, 1280, 1, 1)\n",
    "\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        #print(\"==========before avgpool  Size\", x.size())   ## [수정] CIFAR-10 에서는 여기까지오면 (-, 1280, 7, 7) 이 아니라 (-, ,1280, 1, 1)\n",
    "        x = self.avgpool(x)\n",
    "        #         print(\"==========after avgpool  Size\", x.size())\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_mat[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2) build RWNN\n",
    "channels = 109\n",
    "NN_model = RWNN(net_type='small', graphs=graphs, channels=109,gmats=gmats ,num_classes=10, input_channel=3)\n",
    "NN_model.cuda()\n",
    "\n",
    "###########################\n",
    "# Flops 계산 - [Debug] nn.DataParallele (for multi-gpu) 적용 전에 확인.\n",
    "###########################\n",
    "input_flops = torch.randn(1, input_channel, 32, 32).cuda()\n",
    "flops, params = profile(NN_model, inputs=(input_flops, ), verbose=False) ## 사이즈 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\t - Epoch: [0][0/625]\tTime 0.819 (0.819)\tLoss 2.3352 (2.3352)\tPrec@1 12.500 (12.500)\tPrec@5 58.750 (58.750)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-7801d6c8457d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;31m# train for one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNN_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# evaluate on validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\datascience\\git_repo\\RWNN-optimization\\utils_kyy\\train_validate.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, criterion, optimizer, lr_scheduler, epoch, print_freq, log_file_name)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;31m# measure elapsed time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\newrw\\lib\\site-packages\\torch\\optim\\sgd.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     99\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'momentum_buffer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                         \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Model summary\n",
    "#summary(NN_model, input_size=(1, 224, 224))\n",
    "\n",
    "# 3) Prepare for train### 일단 꺼보자!\n",
    "#NN_model = nn.DataParallel(NN_model)  # for multi-GPU\n",
    "#NN_model = nn.DataParallel(NN_model, device_ids=[0,1,2,3])\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(NN_model.parameters(), args_train.base_lr,\n",
    "                            momentum=args_train.momentum,\n",
    "                            weight_decay=args_train.weight_decay)\n",
    "\n",
    "start_epoch  = 0\n",
    "best_prec1 = 0    \n",
    "\n",
    "cudnn.benchmark = True    # This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware.  \n",
    "\n",
    "###########################\n",
    "# Dataset & Dataloader\n",
    "###########################\n",
    "\n",
    "# 이미 다운 받아놨으니 download=False\n",
    "# 데이터가 없을 경우, 처음에는 download=True 로 설정해놓고 실행해주어야함\n",
    "\n",
    "if data_path is None :\n",
    "    data_path = './data'\n",
    "\n",
    "\n",
    "if args_train.data == \"CIFAR10\" :\n",
    "\n",
    "    cutout_length = 16  # from nsga-net github\n",
    "\n",
    "    CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]  # from nsga-net github\n",
    "    CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "        ])\n",
    "\n",
    "    val_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "        ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=data_path, train=True,\n",
    "                                            download=True, transform=train_transform)\n",
    "\n",
    "    val_dataset = torchvision.datasets.CIFAR10(root=data_path, train=False,\n",
    "                                           download=True, transform=val_transform)\n",
    "\n",
    "else :\n",
    "    raise Exception(\"Data Error, Only CIFAR10 allowed for the moment\")\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args_train.batch_size,\n",
    "                                          shuffle=True, num_workers=args_train.workers)  \n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args_train.batch_size,\n",
    "                                         shuffle=False, num_workers=args_train.workers)    \n",
    "\n",
    "###########################\n",
    "# Train\n",
    "###########################\n",
    "niters = len(train_loader)\n",
    "niters = 1\n",
    "\n",
    "lr_scheduler = LRScheduler(optimizer, niters, args_train)  # (default) args.step = [30, 60, 90], args.decay_factor = 0.1, args.power = 2.0\n",
    "epoch_ = 0\n",
    "\n",
    "#for epoch in range(start_epoch, args_train.epochs):\n",
    "for epoch in range(start_epoch, 30):\n",
    "\n",
    "    # train for one epoch\n",
    "    train(train_loader, NN_model, criterion, optimizer, lr_scheduler, epoch, args_train.print_freq, log_file_name)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec1 = validate(val_loader, NN_model, criterion, epoch, log_file_name)\n",
    "\n",
    "    # remember best prec@1 and save checkpoint\n",
    "#         is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "\n",
    "    epoch_ = epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training 성공 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSGA 적용\n",
    "* Initialization  - WS으로 Graph 및 initialization 생성\n",
    "* Crossover - gray encodng 처럼 같은 stage 안에서만\n",
    "* Mutation - 요거는 생각해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import base, creator\n",
    "from deap import tools\n",
    "\n",
    "\n",
    "import random\n",
    "from itertools import repeat\n",
    "from collections import Sequence\n",
    "\n",
    "# For evaluate function --------------------------\n",
    "import glob\n",
    "from easydict import EasyDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn    # for hardware tunning (cudnn.benchmark = True)\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "import logging\n",
    "\n",
    "# Gray code package\n",
    "from utils_kyy.utils_graycode_v2 import *\n",
    "\n",
    "# custom package in utils_kyy\n",
    "from utils_kyy.utils_graph import load_graph\n",
    "from utils_kyy.models_hr import RWNN\n",
    "from utils_kyy.train_validate import train, validate, train_AMP\n",
    "from utils_kyy.lr_scheduler import LRScheduler\n",
    "from torchsummary import summary\n",
    "# -------------------------------------------------\n",
    "\n",
    "#from apex import amp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage_pool_path_list ~ graphs 만들어주는 과정 -> indivudal에서 바로 생성하는 걸로!\n",
    "\n",
    "\n",
    "def evaluate_hr_full_train(individual, args_train,data_path=args_train.data_path, channels=109, log_file_name=None):  # individual\n",
    "\n",
    "    graphs = []\n",
    "    gmats = []\n",
    "    \n",
    "    for ind in individual:\n",
    "        gmat = ind2gmat(ind, args_train.nsize)\n",
    "        gmats.append(gmat)\n",
    "        graphs.append(gmat2graph(gmat))\n",
    "\n",
    "    \n",
    "    graphs = EasyDict({'stage_1': graphs[0],\n",
    "                       'stage_2': graphs[1],\n",
    "                       'stage_3': graphs[2]\n",
    "                      })\n",
    "\n",
    "    # 2) build RWNN\n",
    "    channels = channels\n",
    "    NN_model = RWNN(net_type='small', graphs=graphs,gmats=gmats ,channels=channels, num_classes=args_train.num_classes, input_channel=args_train.input_dim)\n",
    "    NN_model.cuda()\n",
    "\n",
    "    ###########################\n",
    "    # Flops 계산 - [Debug] nn.DataParallele (for multi-gpu) 적용 전에 확인.\n",
    "    ###########################\n",
    "    input_flops = torch.randn(1, args_train.input_dim, 32, 32).cuda()\n",
    "    flops, params = profile(NN_model, inputs=(input_flops, ), verbose=False)\n",
    "\n",
    "    ## Model summary\n",
    "    #summary(NN_model, input_size=(1, 224, 224))\n",
    "\n",
    "    # 3) Prepare for train### 일단 꺼보자!\n",
    "    #NN_model = nn.DataParallel(NN_model)  # for multi-GPU\n",
    "    #NN_model = nn.DataParallel(NN_model, device_ids=[0,1,2,3])\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD(NN_model.parameters(), args_train.base_lr,\n",
    "                                momentum=args_train.momentum,\n",
    "                                weight_decay=args_train.weight_decay)\n",
    "    \n",
    "    start_epoch  = 0\n",
    "    best_prec1 = 0    \n",
    "    \n",
    "    cudnn.benchmark = True    # This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware.  \n",
    "    \n",
    "    ###########################\n",
    "    # Dataset & Dataloader\n",
    "    ###########################\n",
    "\n",
    "    # 이미 다운 받아놨으니 download=False\n",
    "    # 데이터가 없을 경우, 처음에는 download=True 로 설정해놓고 실행해주어야함\n",
    "    \n",
    "    if data_path is None :\n",
    "        data_path = './data'\n",
    "    \n",
    " \n",
    "    if args_train.data == \"CIFAR10\" :\n",
    "\n",
    "        cutout_length = 16  # from nsga-net github\n",
    "        \n",
    "        CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]  # from nsga-net github\n",
    "        CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "        \n",
    "        train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "              #  Cutout(cutout_length),  # from nsga-net github\n",
    "                transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "            ])\n",
    "\n",
    "        val_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "            ])\n",
    "\n",
    "        train_dataset = torchvision.datasets.CIFAR10(root=data_path, train=True,\n",
    "                                                download=True, transform=train_transform)\n",
    "\n",
    "        val_dataset = torchvision.datasets.CIFAR10(root=data_path, train=False,\n",
    "                                               download=True, transform=val_transform)\n",
    "        \n",
    "    else :\n",
    "        raise Exception(\"Data Error, Only CIFAR10 allowed for the moment\")\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args_train.batch_size,\n",
    "                                              shuffle=True, num_workers=args_train.workers)  \n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args_train.batch_size,\n",
    "                                             shuffle=False, num_workers=args_train.workers)    \n",
    "    \n",
    "    ###########################\n",
    "    # Train\n",
    "    ###########################\n",
    "    niters = len(train_loader)\n",
    "    niters = 1\n",
    "\n",
    "    lr_scheduler = LRScheduler(optimizer, niters, args_train)  # (default) args.step = [30, 60, 90], args.decay_factor = 0.1, args.power = 2.0\n",
    "    epoch_ = 0\n",
    "    \n",
    "    for epoch in range(start_epoch, args_train.epochs):\n",
    "        # train for one epoch\n",
    "        train(train_loader, NN_model, criterion, optimizer, lr_scheduler, epoch, args_train.print_freq, log_file_name)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, NN_model, criterion, epoch, log_file_name)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "#         is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        \n",
    "        epoch_ = epoch\n",
    "\n",
    "    return (-best_prec1, flops), epoch_  # Min (-val_accuracy, flops) 이므로 val_accuracy(top1)에 - 붙여서 return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Conv1 Size torch.Size([1, 54, 32, 32])\n",
      "==========Conv2  Size torch.Size([1, 109, 32, 32])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'oplist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-e9bdbd086cf9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnds\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mget_graph_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrph_ex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mindividuals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgmat2ind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_g_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfitness_value\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_hr_full_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindividuals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0margs_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-2bf5aac690c9>\u001b[0m in \u001b[0;36mevaluate_hr_full_train\u001b[1;34m(individual, args_train, data_path, channels, log_file_name)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m###########################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0minput_flops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mflops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNN_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_flops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m## Model summary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\newrw\\lib\\site-packages\\thop\\profile.py\u001b[0m in \u001b[0;36mprofile\u001b[1;34m(model, inputs, custom_ops, verbose)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mtotal_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\newrw\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\datascience\\git_repo\\RWNN-optimization\\utils_kyy\\models_hr.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"==========Conv2  Size\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# => (-, 109, 8, 8)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"==========Conv3  Size\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# => (-, 109, 8, 8)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\newrw\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\datascience\\git_repo\\RWNN-optimization\\utils_kyy\\models_hr.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[1;31m# 따라서, input으로 넣을 때 unpack 함.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;31m# id 작은 노드부터 result를 차근차근 계산하면서, id를 올라감.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodeop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\newrw\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\datascience\\git_repo\\RWNN-optimization\\utils_kyy\\models_hr.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moplist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m## input node가 아니라고 생각해도 됨!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_nums\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"input index {} Operation Type {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moplist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moplist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'oplist' is not defined"
     ]
    }
   ],
   "source": [
    "# sample individuals to train\n",
    "\n",
    "nds , _, _  = get_graph_info(grph_ex)\n",
    "individuals = [gmat2ind(get_g_matrix(nds)) for i in range(3)]\n",
    "fitness_value , epoch = evaluate_hr_full_train(individuals,args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fitness_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-df90c6e08d08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Check evaluation funtion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfitness_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'fitness_value' is not defined"
     ]
    }
   ],
   "source": [
    "## Check evaluation funtion\n",
    "print(fitness_value, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tool box for hr\n",
    "# individual 바뀌어야\n",
    "\n",
    "def create_toolbox_for_NSGA_RWNN(num_graph, args_train, stage_pool_path, data_path=None ,log_file_name=None):\n",
    "    # => Min ( -val_accuracy(top_1),  flops )\n",
    "    creator.create('FitnessMin', base.Fitness, weights=(-1.0, -1.0 ))  # name, base (class), attribute // \n",
    "    creator.create('Individual', list, fitness=creator.FitnessMin)  # creator.FitnessMaxMin attribute로 가짐    \n",
    "    \n",
    "    #####################################\n",
    "    # Initialize the toolbox\n",
    "    #####################################\n",
    "    toolbox = base.Toolbox()\n",
    "    if args_train.graycode :\n",
    "        gray_len = len(str(grayCode(num_graph-1)))\n",
    "        IND_SIZE = gray_len * 3\n",
    "        BOUND_LOW = 0\n",
    "        BOUND_UP = 1\n",
    "        toolbox.register('attr_int', random.randint, BOUND_LOW, BOUND_UP)\n",
    "\n",
    "    else:\n",
    "        IND_SIZE = 3    # 한 individual, 즉 하나의 chromosome은 3개의 graph. 즉, 3개의 stage를 가짐.\n",
    "\n",
    "        # toolbox.attribute(0, (num_graph-1)) 이렇게 사용함.\n",
    "        # 즉, 0 ~ (num_grpah - 1) 중 임의의 정수 선택. => 이걸 3번하면 하나의 small graph가 생김\n",
    "        BOUND_LOW = 0\n",
    "        BOUND_UP = num_graph-1\n",
    "        toolbox.register('attr_int', random.randint, BOUND_LOW, BOUND_UP)   # register(alias, method, argument ...)\n",
    "        # toolbox.attribute라는 함수를 n번 시행해서 containter인 creator.individual에 넣은 후 해당 instance를 반환함.\n",
    "        # e.g. [0, 1, 3] 반환\n",
    "    toolbox.register('individual', tools.initRepeat,\n",
    "                     creator.Individual, toolbox.attr_int, n=IND_SIZE)\n",
    "\n",
    "    toolbox.register('population', tools.initRepeat,\n",
    "                     list, toolbox.individual)    # n은 생략함. toolbox.population 함수를 뒤에서 실행할 때 넣어줌.    \n",
    "    \n",
    "    # crossover\n",
    "    if args_train.graycode :\n",
    "        toolbox.register('mate', cxgray, num_graph=num_graph)\n",
    "    else:\n",
    "        toolbox.register('mate', tools.cxTwoPoint)  # crossover\n",
    "\n",
    "    # mutation\n",
    "    toolbox.register('mutate', mutUniformInt_custom, low=BOUND_LOW, up=BOUND_UP)\n",
    "\n",
    "    # selection\n",
    "    # => return A list of selected individuals.\n",
    "    toolbox.register('select', tools.selNSGA2, nd='standard')  # selection.  // k – The number of individuals to select. k는 함수 쓸 때 받아야함\n",
    "    \n",
    "    #########################\n",
    "    # Seeding a population - train_log 읽어와서 해당 log의 마지막 population으로 init 후 이어서 train 시작\n",
    "    #########################\n",
    "    # [Reference] https://deap.readthedocs.io/en/master/tutorials/basic/part1.html\n",
    "    def LoadIndividual(icls, content):\n",
    "        return icls(content)\n",
    "\n",
    "    def LoadPopulation(pcls, ind_init, last_population):  # list of [chromosome, [-val_accuracy, flops]]\n",
    "        return pcls(ind_init(last_population[i][0]) for i in range(len(last_population)))\n",
    "\n",
    "    toolbox.register(\"individual_load\", LoadIndividual, creator.Individual)\n",
    "\n",
    "    toolbox.register(\"population_load\", LoadPopulation, list, toolbox.individual_load)\n",
    "    \n",
    "    return toolbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'newrw'",
   "language": "python",
   "name": "newrw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
