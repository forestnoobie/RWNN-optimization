{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hr Neural Network Train Test\n",
    "* stage block 구성 완료\n",
    "* 학습 가능한지 점검"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* toolbox 구성\n",
    "* 임의로 graphs 형성 (불러오기 하지말고 바로 넘겨 받을 수 있게 설정)\n",
    "* 학습 및 acc 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import base, creator\n",
    "from deap import tools\n",
    "\n",
    "\n",
    "import random\n",
    "from itertools import repeat\n",
    "from collections import Sequence\n",
    "\n",
    "# For evaluate function --------------------------\n",
    "import glob\n",
    "from easydict import EasyDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn    # for hardware tunning (cudnn.benchmark = True)\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "import logging\n",
    "\n",
    "# Gray code package\n",
    "from utils_kyy.utils_graycode_v2 import *\n",
    "\n",
    "# custom package in utils_kyy\n",
    "from utils_kyy.utils_graph import load_graph\n",
    "from utils_kyy.models import RWNN\n",
    "from utils_kyy.train_validate import train, validate, train_AMP\n",
    "from utils_kyy.lr_scheduler import LRScheduler\n",
    "from torchsummary import summary\n",
    "# -------------------------------------------------\n",
    "\n",
    "#from apex import amp\n",
    "\n",
    "import time\n",
    "\n",
    "## For MNIST\n",
    "class ReshapeTransform:\n",
    "    def __init__(self, new_size):\n",
    "        self.new_size = new_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return torch.reshape(img, self.new_size)\n",
    "\n",
    "# create the toolbox with the right parameters\n",
    "def create_toolbox_for_NSGA_RWNN(num_graph, args_train, stage_pool_path, data_path=None ,log_file_name=None):\n",
    "    # => Min ( -val_accuracy(top_1),  flops )\n",
    "    creator.create('FitnessMin', base.Fitness, weights=(-1.0, -1.0 ))  # name, base (class), attribute // \n",
    "    creator.create('Individual', list, fitness=creator.FitnessMin)  # creator.FitnessMaxMin attribute로 가짐    \n",
    "    \n",
    "    #####################################\n",
    "    # Initialize the toolbox\n",
    "    #####################################\n",
    "    toolbox = base.Toolbox()\n",
    "    if args_train.graycode :\n",
    "        gray_len = len(str(grayCode(num_graph-1)))\n",
    "        IND_SIZE = gray_len * 3\n",
    "        BOUND_LOW = 0\n",
    "        BOUND_UP = 1\n",
    "        toolbox.register('attr_int', random.randint, BOUND_LOW, BOUND_UP)\n",
    "\n",
    "    else:\n",
    "        IND_SIZE = 3    # 한 individual, 즉 하나의 chromosome은 3개의 graph. 즉, 3개의 stage를 가짐.\n",
    "\n",
    "        # toolbox.attribute(0, (num_graph-1)) 이렇게 사용함.\n",
    "        # 즉, 0 ~ (num_grpah - 1) 중 임의의 정수 선택. => 이걸 3번하면 하나의 small graph가 생김\n",
    "        BOUND_LOW = 0\n",
    "        BOUND_UP = num_graph-1\n",
    "        toolbox.register('attr_int', random.randint, BOUND_LOW, BOUND_UP)   # register(alias, method, argument ...)\n",
    "        # toolbox.attribute라는 함수를 n번 시행해서 containter인 creator.individual에 넣은 후 해당 instance를 반환함.\n",
    "        # e.g. [0, 1, 3] 반환\n",
    "    toolbox.register('individual', tools.initRepeat,\n",
    "                     creator.Individual, toolbox.attr_int, n=IND_SIZE)\n",
    "\n",
    "    toolbox.register('population', tools.initRepeat,\n",
    "                     list, toolbox.individual)    # n은 생략함. toolbox.population 함수를 뒤에서 실행할 때 넣어줌.    \n",
    "    \n",
    "    # crossover\n",
    "    if args_train.graycode :\n",
    "        toolbox.register('mate', cxgray, num_graph=num_graph)\n",
    "    else:\n",
    "        toolbox.register('mate', tools.cxTwoPoint)  # crossover\n",
    "\n",
    "    # mutation\n",
    "    toolbox.register('mutate', mutUniformInt_custom, low=BOUND_LOW, up=BOUND_UP)\n",
    "\n",
    "    # selection\n",
    "    # => return A list of selected individuals.\n",
    "    toolbox.register('select', tools.selNSGA2, nd='standard')  # selection.  // k – The number of individuals to select. k는 함수 쓸 때 받아야함\n",
    "    \n",
    "    #########################\n",
    "    # Seeding a population - train_log 읽어와서 해당 log의 마지막 population으로 init 후 이어서 train 시작\n",
    "    #########################\n",
    "    # [Reference] https://deap.readthedocs.io/en/master/tutorials/basic/part1.html\n",
    "    def LoadIndividual(icls, content):\n",
    "        return icls(content)\n",
    "\n",
    "    def LoadPopulation(pcls, ind_init, last_population):  # list of [chromosome, [-val_accuracy, flops]]\n",
    "        return pcls(ind_init(last_population[i][0]) for i in range(len(last_population)))\n",
    "\n",
    "    toolbox.register(\"individual_load\", LoadIndividual, creator.Individual)\n",
    "\n",
    "    toolbox.register(\"population_load\", LoadPopulation, list, toolbox.individual_load)\n",
    "    \n",
    "    return toolbox\n",
    "\n",
    "def evaluate_v2_full_train(individual, args_train, stage_pool_path_list, data_path=None, channels=109, log_file_name=None):  # individual\n",
    "    \n",
    "    # 1) load graph\n",
    "    total_graph_path_list = []\n",
    "    for i in range(3):\n",
    "        temp = glob.glob(stage_pool_path_list[i] + '*.yaml') # sorting 해줘야함\n",
    "        temp.sort()\n",
    "        total_graph_path_list.append( temp )        \n",
    "#         total_graph_path_list.append( glob.glob(stage_pool_path_list[i] + '*.yaml') )\n",
    "\n",
    "    graph_name = []\n",
    "\n",
    "    # args_train 셋팅에서 graycode 변환이 true 인지 확인\n",
    "    if args_train.graycode:\n",
    "        ## Decode 해줘야 !\n",
    "        gray_len = len(individual)//3\n",
    "        for i in range(3):\n",
    "            # list to string\n",
    "            tmp = ''\n",
    "            for j in individual[gray_len*i:gray_len*(i+1)]:\n",
    "                tmp += str(j)\n",
    "\n",
    "            # sting to binary to num\n",
    "            graph_name.append(graydecode(int(tmp)))\n",
    "\n",
    "    else :\n",
    "        graph_name = individual\n",
    "\n",
    "    stage_1_graph = load_graph( total_graph_path_list[0][graph_name[0]] )\n",
    "    stage_2_graph = load_graph( total_graph_path_list[1][graph_name[1]] )\n",
    "    stage_3_graph = load_graph( total_graph_path_list[2][graph_name[2]] )\n",
    "    \n",
    "    graphs = EasyDict({'stage_1': stage_1_graph,\n",
    "                       'stage_2': stage_2_graph,\n",
    "                       'stage_3': stage_3_graph\n",
    "                      })\n",
    "\n",
    "    # 2) build RWNN\n",
    "    channels = channels\n",
    "    NN_model = RWNN(net_type='small', graphs=graphs, channels=channels, num_classes=args_train.num_classes, input_channel=args_train.input_dim)\n",
    "    NN_model.cuda()\n",
    "\n",
    "    ###########################\n",
    "    # Flops 계산 - [Debug] nn.DataParallele (for multi-gpu) 적용 전에 확인.\n",
    "    ###########################\n",
    "    input_flops = torch.randn(1, args_train.input_dim, 32, 32).cuda()\n",
    "    flops, params = profile(NN_model, inputs=(input_flops, ), verbose=False)\n",
    "\n",
    "    ## Model summary\n",
    "    #summary(NN_model, input_size=(1, 224, 224))\n",
    "\n",
    "    # 3) Prepare for train### 일단 꺼보자!\n",
    "    #NN_model = nn.DataParallel(NN_model)  # for multi-GPU\n",
    "    NN_model = nn.DataParallel(NN_model, device_ids=[0,1,2,3])\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD(NN_model.parameters(), args_train.base_lr,\n",
    "                                momentum=args_train.momentum,\n",
    "                                weight_decay=args_train.weight_decay)\n",
    "    \n",
    "    start_epoch  = 0\n",
    "    best_prec1 = 0    \n",
    "    \n",
    "    cudnn.benchmark = True    # This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware.  \n",
    "    \n",
    "    ###########################\n",
    "    # Dataset & Dataloader\n",
    "    ###########################\n",
    "\n",
    "    # 이미 다운 받아놨으니 download=False\n",
    "    # 데이터가 없을 경우, 처음에는 download=True 로 설정해놓고 실행해주어야함\n",
    "    \n",
    "    if data_path is None :\n",
    "        data_path = './data'\n",
    "    \n",
    " \n",
    "    if args_train.data == \"CIFAR10\" :\n",
    "\n",
    "        cutout_length = 16  # from nsga-net github\n",
    "        \n",
    "        CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]  # from nsga-net github\n",
    "        CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "        \n",
    "        train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                Cutout(cutout_length),  # from nsga-net github\n",
    "                transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "            ])\n",
    "\n",
    "        val_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "            ])\n",
    "\n",
    "        train_dataset = torchvision.datasets.CIFAR10(root=data_path, train=True,\n",
    "                                                download=True, transform=train_transform)\n",
    "\n",
    "        val_dataset = torchvision.datasets.CIFAR10(root=data_path, train=False,\n",
    "                                               download=True, transform=val_transform)\n",
    "        \n",
    "    else :\n",
    "        raise Exception(\"Data Error, Only CIFAR10 allowed for the moment\")\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args_train.batch_size,\n",
    "                                              shuffle=True, num_workers=args_train.workers)  \n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args_train.batch_size,\n",
    "                                             shuffle=False, num_workers=args_train.workers)    \n",
    "    \n",
    "    ###########################\n",
    "    # Train\n",
    "    ###########################\n",
    "    niters = len(train_loader)\n",
    "    niters = 1\n",
    "\n",
    "    lr_scheduler = LRScheduler(optimizer, niters, args_train)  # (default) args.step = [30, 60, 90], args.decay_factor = 0.1, args.power = 2.0\n",
    "    epoch_ = 0\n",
    "    \n",
    "    for epoch in range(start_epoch, args_train.epochs):\n",
    "        # train for one epoch\n",
    "        train(train_loader, NN_model, criterion, optimizer, lr_scheduler, epoch, args_train.print_freq, log_file_name)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, NN_model, criterion, epoch, log_file_name)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "#         is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        \n",
    "        epoch_ = epoch\n",
    "\n",
    "    return (-best_prec1, flops), epoch_  # Min (-val_accuracy, flops) 이므로 val_accuracy(top1)에 - 붙여서 return\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "# Mutate\n",
    "############################\n",
    "# 기존 mutUniformInt에 xrange() 함수가 사용됐어서, range로 수정함.\n",
    "# indpb: toolbox.mutate() 함수로 사용할 때, MUTPB로 넣어줌. individual의 각 원소에 mutation 적용될 확률.\n",
    "# indpb – Independent probability for each attribute to be mutated.\n",
    "def mutUniformInt_custom(individual, low, up, indpb):\n",
    "    \"\"\"Mutate an individual by replacing attributes, with probability *indpb*,\n",
    "    by a integer uniformly drawn between *low* and *up* inclusively.\n",
    "    :param individual: :term:`Sequence <sequence>` individual to be mutated.\n",
    "    :param low: The lower bound or a :term:`python:sequence` of\n",
    "                of lower bounds of the range from wich to draw the new\n",
    "                integer.\n",
    "    :param up: The upper bound or a :term:`python:sequence` of\n",
    "               of upper bounds of the range from wich to draw the new\n",
    "               integer.\n",
    "    :param indpb: Independent probability for each attribute to be mutated.\n",
    "    :returns: A tuple of one individual.\n",
    "    \"\"\"\n",
    "    size = len(individual)\n",
    "    if not isinstance(low, Sequence):\n",
    "        low = repeat(low, size)\n",
    "    elif len(low) < size:\n",
    "        raise IndexError(\"low must be at least the size of individual: %d < %d\" % (len(low), size))\n",
    "    if not isinstance(up, Sequence):\n",
    "        up = repeat(up, size)\n",
    "    elif len(up) < size:\n",
    "        raise IndexError(\"up must be at least the size of individual: %d < %d\" % (len(up), size))\n",
    "\n",
    "    for i, xl, xu in zip(range(size), low, up):\n",
    "        if random.random() < indpb:\n",
    "            individual[i] = random.randint(xl, xu)\n",
    "\n",
    "    return individual,\n",
    "\n",
    "\n",
    "############################\n",
    "# Evaluate\n",
    "############################\n",
    "\"\"\"\n",
    "# fitness function\n",
    "    input: [0, 5, 10]   하나의 크로모좀.\n",
    "\n",
    "    1) input인 [0, 5, 10]을 받아서 (0번째, 5번째, 10번째)에 해당하는 그래프 파일 각각 읽어와서 신경망 구축\n",
    "    \n",
    "    2) training (임시로 1 epoch. 실제 실험 시, RWNN과 같은 epoch 학습시키기)\n",
    "    \n",
    "    3) return flops, val_accuracy\n",
    "\n",
    "\"\"\"\n",
    "def evaluate(individual, args_train, stage_pool_path, data_path=None ,channels=109, log_file_name=None):  # individual\n",
    "\n",
    "    # list 형식의 individual 객체를 input으로 받음   e.g. [0, 4, 17]\n",
    "    # 1) load graph\n",
    "    total_graph_path = glob.glob(stage_pool_path + '*.yaml')    # list\n",
    "\n",
    "    graph_name = []\n",
    "\n",
    "    # args_train 셋팅에서 graycode 변환이 true 인지 확인\n",
    "    if args_train.graycode:\n",
    "        ## Decode 해줘야 !\n",
    "        gray_len = len(individual)//3\n",
    "        for i in range(3):\n",
    "            # list to string\n",
    "            tmp = ''\n",
    "            for j in individual[gray_len*i:gray_len*(i+1)]:\n",
    "                tmp += str(j)\n",
    "\n",
    "            # sting to binary to num\n",
    "            graph_name.append(graydecode(int(tmp)))\n",
    "\n",
    "    else :\n",
    "        graph_name = individual\n",
    "\n",
    "    stage_1_graph = load_graph(total_graph_path[graph_name[0]])\n",
    "    stage_2_graph = load_graph(total_graph_path[graph_name[1]])\n",
    "    stage_3_graph = load_graph(total_graph_path[graph_name[2]])\n",
    "    \n",
    "    graphs = EasyDict({'stage_1': stage_1_graph,\n",
    "                       'stage_2': stage_2_graph,\n",
    "                       'stage_3': stage_3_graph\n",
    "                      })\n",
    "\n",
    "    # 2) build RWNN\n",
    "    channels = channels\n",
    "    NN_model = RWNN(net_type='small', graphs=graphs, channels=channels, num_classes=args_train.num_classes, input_channel=args_train.input_dim)\n",
    "    NN_model.cuda()\n",
    "\n",
    "    ###########################\n",
    "    # Flops 계산 - [Debug] nn.DataParallele (for multi-gpu) 적용 전에 확인.\n",
    "    ###########################\n",
    "    input_flops = torch.randn(1, args_train.input_dim, 32, 32).cuda()\n",
    "    flops, params = profile(NN_model, inputs=(input_flops, ), verbose=False)\n",
    "\n",
    "    ## Model summary\n",
    "    #summary(NN_model, input_size=(1, 224, 224))\n",
    "\n",
    "    # 3) Prepare for train### 일단 꺼보자!\n",
    "    #NN_model = nn.DataParallel(NN_model)  # for multi-GPU\n",
    "    NN_model = nn.DataParallel(NN_model, device_ids=[0,1,2,3])\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD(NN_model.parameters(), args_train.base_lr,\n",
    "                                momentum=args_train.momentum,\n",
    "                                weight_decay=args_train.weight_decay)\n",
    "    \n",
    "    start_epoch  = 0\n",
    "    best_prec1 = 0    \n",
    "    \n",
    "    cudnn.benchmark = True    # This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware.  \n",
    "    \n",
    "    ###########################\n",
    "    # Dataset & Dataloader\n",
    "    ###########################\n",
    "\n",
    "\n",
    "    # 이미 다운 받아놨으니 download=False\n",
    "    # 데이터가 없을 경우, 처음엔느 download=True 로 설정해놓고 실행해주어야함\n",
    "    \n",
    "    if data_path is None :\n",
    "        data_path = './data'\n",
    "    \n",
    " \n",
    "    if args_train.data == \"CIFAR10\" :\n",
    "\n",
    "        cutout_length = 16  # from nsga-net github\n",
    "        \n",
    "        CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]  # from nsga-net github\n",
    "        CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "        \n",
    "        train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                Cutout(cutout_length),  # from nsga-net github\n",
    "                transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "            ])\n",
    "\n",
    "        val_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "            ])\n",
    "\n",
    "        train_dataset = torchvision.datasets.CIFAR10(root=data_path, train=True,\n",
    "                                                download=True, transform=train_transform)\n",
    "\n",
    "        val_dataset = torchvision.datasets.CIFAR10(root=data_path, train=False,\n",
    "                                               download=True, transform=val_transform)\n",
    "\n",
    "        \n",
    "    else :\n",
    "        raise Exception(\"Data Error, Only CIFAR10 allowed for the moment\")\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args_train.batch_size,\n",
    "                                              shuffle=True, num_workers=args_train.workers)  \n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args_train.batch_size,\n",
    "                                             shuffle=False, num_workers=args_train.workers)    \n",
    "    \n",
    "    ###########################\n",
    "    # Train\n",
    "    ###########################\n",
    "    niters = len(train_loader)\n",
    "    #niters = 1\n",
    "\n",
    "    lr_scheduler = LRScheduler(optimizer, niters, args_train)  # (default) args.step = [30, 60, 90], args.decay_factor = 0.1, args.power = 2.0    \n",
    "    \n",
    "    for epoch in range(start_epoch, args_train.epochs):\n",
    "        # train for one epoch\n",
    "        train(train_loader, NN_model, criterion, optimizer, lr_scheduler, epoch, args_train.print_freq, log_file_name)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, NN_model, criterion, epoch, log_file_name)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "#         is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "\n",
    "    return (-best_prec1, flops), NN_model  # Min (-val_accuracy, flops) 이므로 val_accuracy(top1)에 - 붙여서 return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "# evaluate_v3_APEX : GPU 병렬처리 - NVIDIA-APEX \n",
    "############################\n",
    "def evaluate_v3_APEX(individual, args_train, stage_pool_path_list, data_path=None ,channels=109, log_file_name=None):  # individual\n",
    "\n",
    "    ## GPU 병렬처리 - APEX\n",
    "    torch.distributed.init_process_group(backend='nccl', init_method='env://')   \n",
    "    cudnn.benchmark = True    # This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware.  \n",
    "    \n",
    "    # list 형식의 individual 객체를 input으로 받음   e.g. [0, 4, 17]\n",
    "    # 1) load graph\n",
    "    total_graph_path_list = []\n",
    "    for i in range(3):\n",
    "        total_graph_path_list.append( glob.glob(stage_pool_path_list[i] + '*.yaml') )\n",
    "\n",
    "    graph_name = []\n",
    "\n",
    "    # args_train 셋팅에서 graycode 변환이 true 인지 확인\n",
    "    if args_train.graycode:\n",
    "        ## Decode 해줘야 !\n",
    "        gray_len = len(individual)//3\n",
    "        for i in range(3):\n",
    "            # list to string\n",
    "            tmp = ''\n",
    "            for j in individual[gray_len*i:gray_len*(i+1)]:\n",
    "                tmp += str(j)\n",
    "\n",
    "            # sting to binary to num\n",
    "            graph_name.append(graydecode(int(tmp)))\n",
    "\n",
    "    else :\n",
    "        graph_name = individual\n",
    "\n",
    "    stage_1_graph = load_graph( total_graph_path_list[0][graph_name[0]] )\n",
    "    stage_2_graph = load_graph( total_graph_path_list[1][graph_name[1]] )\n",
    "    stage_3_graph = load_graph( total_graph_path_list[2][graph_name[2]] )\n",
    "    \n",
    "    graphs = EasyDict({'stage_1': stage_1_graph,\n",
    "                       'stage_2': stage_2_graph,\n",
    "                       'stage_3': stage_3_graph\n",
    "                      })\n",
    "\n",
    "    # 2) build RWNN\n",
    "    channels = channels\n",
    "    NN_model = RWNN(net_type='small', graphs=graphs, channels=channels, num_classes=args_train.num_classes, input_channel=args_train.input_dim)\n",
    "    NN_model.cuda()\n",
    "\n",
    "    ###########################\n",
    "    # Flops 계산 - [Debug] nn.DataParallele (for multi-gpu) 적용 전에 확인.\n",
    "    ###########################\n",
    "    input_flops = torch.randn(1, args_train.input_dim, 224, 224).cuda()\n",
    "    flops, params = profile(NN_model, inputs=(input_flops, ), verbose=False)\n",
    "\n",
    "    ## Model summary\n",
    "    #summary(NN_model, input_size=(1, 224, 224))\n",
    "\n",
    "    # 3) Prepare for train\n",
    "#     NN_model = nn.DataParallel(NN_model, device_ids=[0,1,2,3])  # amp.initialize 한 후에 nn.DataParallel 로 wrap 해야함.\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD(NN_model.parameters(), args_train.base_lr,\n",
    "                                momentum=args_train.momentum,\n",
    "                                weight_decay=args_train.weight_decay)\n",
    "\n",
    "    # AMP initialization\n",
    "    # [Reference] https://github.com/NVIDIA/apex/tree/master/examples/dcgan\n",
    "    # opt_level default setting from API guide = 'O1'\n",
    "    [NN_model], [optimizer] = amp.initialize([NN_model], [optimizer], opt_level='O1', num_losses=1)\n",
    "    \n",
    "#     NN_model = nn.DataParallel(NN_model, device_ids=[0,1,2,3])\n",
    "    NN_model = DDP(NN_model)\n",
    "    \n",
    "    start_epoch  = 0\n",
    "    best_prec1 = 0\n",
    "    \n",
    "    ###########################\n",
    "    # Dataset & Dataloader\n",
    "    ###########################\n",
    "\n",
    "    # 이미 다운 받아놨으니 download=False\n",
    "    # 데이터가 없을 경우, 처음에는 download=True 로 설정해놓고 실행해주어야함\n",
    "    \n",
    "    if data_path is None :\n",
    "        data_path = './data'\n",
    "    \n",
    " \n",
    "    if args_train.data == \"CIFAR10\" :\n",
    "\n",
    "        train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomHorizontalFlip(),  # 추가함\n",
    "                transforms.Resize(224),  # 추가함.  imagenet dataset과 size 맞추기\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # rescale 0 ~ 1 => -1 ~ 1\n",
    "            ])\n",
    "\n",
    "        val_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(224),  # 추가함.  imagenet dataset과 size 맞추기\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # rescale 0 ~ 1 => -1 ~ 1\n",
    "            ])\n",
    "\n",
    "        train_dataset = torchvision.datasets.CIFAR10(root=data_path, train=True,\n",
    "                                                download=True, transform=train_transform)\n",
    "\n",
    "        val_dataset = torchvision.datasets.CIFAR10(root=data_path, train=False,\n",
    "                                               download=True, transform=val_transform)\n",
    "    elif args_train.data == \"MNIST\":\n",
    "\n",
    "        train_transform =  transforms.Compose([\n",
    "                                               transforms.Resize(224),\n",
    "                                                transforms.ToTensor(),  # 추가함.  imagenet dataset과 size 맞추기\n",
    "                                               transforms.Normalize((0.5,), (1.0,)),\n",
    "\n",
    "            ])\n",
    "        val_transform = transforms.Compose([transforms.Resize(224),\n",
    "                                            transforms.ToTensor(),# 추가함.  imagenet dataset과 size 맞추기\n",
    "                                            transforms.Normalize((0.5,), (1.0,))\n",
    "            ])\n",
    "        train_dataset = torchvision.datasets.MNIST(root=data_path, train=True, transform=train_transform, download=True)\n",
    "        val_dataset = torchvision.datasets.MNIST(root=data_path, train=False, transform=val_transform, download=True)\n",
    "        \n",
    "    else :\n",
    "        raise Exception(\"Data Error, Only CIFAR10, MNIST allowed for the moment\")\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args_train.batch_size,\n",
    "                                              shuffle=True, num_workers=args_train.workers)  \n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args_train.batch_size,\n",
    "                                             shuffle=False, num_workers=args_train.workers)    \n",
    "    \n",
    "    ###########################\n",
    "    # Train\n",
    "    ###########################\n",
    "    niters = len(train_loader)\n",
    "    #niters = 1\n",
    "\n",
    "    lr_scheduler = LRScheduler(optimizer, niters, args_train)  # (default) args.step = [30, 60, 90], args.decay_factor = 0.1, args.power = 2.0    \n",
    "    \n",
    "    for epoch in range(start_epoch, args_train.epochs):\n",
    "        # train for one epoch\n",
    "        train_AMP(train_loader, NN_model, criterion, optimizer, lr_scheduler, epoch, args_train.print_freq, log_file_name)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, NN_model, criterion, epoch, log_file_name)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "#         is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "\n",
    "\n",
    "    return (-best_prec1, flops), NN_model  # Min (-val_accuracy, flops) 이므로 val_accuracy(top1)에 - 붙여서 return\n",
    "\n",
    "# [Reference] https://github.com/ianwhale/nsga-net/blob/master/misc/utils.py\n",
    "class Cutout(object):\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h, w = img.size(1), img.size(2)\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "\n",
    "        y1 = np.clip(y - self.length // 2, 0, h)\n",
    "        y2 = np.clip(y + self.length // 2, 0, h)\n",
    "        x1 = np.clip(x - self.length // 2, 0, w)\n",
    "        x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "        mask[y1: y2, x1: x2] = 0.\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img *= mask\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'newrw'",
   "language": "python",
   "name": "newrw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
