{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hr Neural Network Train Test\n",
    "* stage block 구성 완료\n",
    "* 학습 가능한지 점검"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* toolbox 구성\n",
    "* 임의로 graphs 형성 (불러오기 하지말고 바로 넘겨 받을 수 있게 설정)\n",
    "* 학습 및 acc 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import base, creator\n",
    "from deap import tools\n",
    "\n",
    "\n",
    "import random\n",
    "from itertools import repeat\n",
    "from collections import Sequence\n",
    "\n",
    "# For evaluate function --------------------------\n",
    "import glob\n",
    "from easydict import EasyDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn    # for hardware tunning (cudnn.benchmark = True)\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "import logging\n",
    "\n",
    "# Gray code package\n",
    "from utils_kyy.utils_graycode_v2 import *\n",
    "\n",
    "# custom package in utils_kyy\n",
    "from utils_kyy.utils_graph import load_graph\n",
    "from utils_kyy.models_hr import RWNN\n",
    "from utils_kyy.train_validate import train, validate, train_AMP\n",
    "from utils_kyy.lr_scheduler import LRScheduler\n",
    "from torchsummary import summary\n",
    "# -------------------------------------------------\n",
    "\n",
    "#from apex import amp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage_pool_path_list ~ graphs 만들어주는 과정 -> indivudal에서 바로 생성하는 걸로!\n",
    "\n",
    "\n",
    "def evaluate_hr_full_train(individual, args_train, stage_pool_path_list, data_path=None, channels=109, log_file_name=None):  # individual\n",
    "    \n",
    "#     # 1) load graph\n",
    "#     total_graph_path_list = []\n",
    "#     for i in range(3):\n",
    "#         temp = glob.glob(stage_pool_path_list[i] + '*.yaml') # sorting 해줘야함\n",
    "#         temp.sort()\n",
    "#         total_graph_path_list.append( temp )        \n",
    "# #         total_graph_path_list.append( glob.glob(stage_pool_path_list[i] + '*.yaml') )\n",
    "    \n",
    "    # grape \n",
    "\n",
    "    graph_name = []\n",
    "\n",
    "    # args_train 셋팅에서 graycode 변환이 true 인지 확인\n",
    "    if args_train.graycode:\n",
    "        ## Decode 해줘야 !\n",
    "        gray_len = len(individual)//3\n",
    "        for i in range(3):\n",
    "            # list to string\n",
    "            tmp = ''\n",
    "            for j in individual[gray_len*i:gray_len*(i+1)]:\n",
    "                tmp += str(j)\n",
    "\n",
    "            # sting to binary to num\n",
    "            graph_name.append(graydecode(int(tmp)))\n",
    "\n",
    "    else :\n",
    "        graph_name = individual\n",
    "\n",
    "    stage_1_graph = load_graph( total_graph_path_list[0][graph_name[0]] )\n",
    "    stage_2_graph = load_graph( total_graph_path_list[1][graph_name[1]] )\n",
    "    stage_3_graph = load_graph( total_graph_path_list[2][graph_name[2]] )\n",
    "    \n",
    "    graphs = EasyDict({'stage_1': stage_1_graph,\n",
    "                       'stage_2': stage_2_graph,\n",
    "                       'stage_3': stage_3_graph\n",
    "                      })\n",
    "\n",
    "    # 2) build RWNN\n",
    "    channels = channels\n",
    "    NN_model = RWNN(net_type='small', graphs=graphs, channels=channels, num_classes=args_train.num_classes, input_channel=args_train.input_dim)\n",
    "    NN_model.cuda()\n",
    "\n",
    "    ###########################\n",
    "    # Flops 계산 - [Debug] nn.DataParallele (for multi-gpu) 적용 전에 확인.\n",
    "    ###########################\n",
    "    input_flops = torch.randn(1, args_train.input_dim, 32, 32).cuda()\n",
    "    flops, params = profile(NN_model, inputs=(input_flops, ), verbose=False)\n",
    "\n",
    "    ## Model summary\n",
    "    #summary(NN_model, input_size=(1, 224, 224))\n",
    "\n",
    "    # 3) Prepare for train### 일단 꺼보자!\n",
    "    #NN_model = nn.DataParallel(NN_model)  # for multi-GPU\n",
    "    NN_model = nn.DataParallel(NN_model, device_ids=[0,1,2,3])\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD(NN_model.parameters(), args_train.base_lr,\n",
    "                                momentum=args_train.momentum,\n",
    "                                weight_decay=args_train.weight_decay)\n",
    "    \n",
    "    start_epoch  = 0\n",
    "    best_prec1 = 0    \n",
    "    \n",
    "    cudnn.benchmark = True    # This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware.  \n",
    "    \n",
    "    ###########################\n",
    "    # Dataset & Dataloader\n",
    "    ###########################\n",
    "\n",
    "    # 이미 다운 받아놨으니 download=False\n",
    "    # 데이터가 없을 경우, 처음에는 download=True 로 설정해놓고 실행해주어야함\n",
    "    \n",
    "    if data_path is None :\n",
    "        data_path = './data'\n",
    "    \n",
    " \n",
    "    if args_train.data == \"CIFAR10\" :\n",
    "\n",
    "        cutout_length = 16  # from nsga-net github\n",
    "        \n",
    "        CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]  # from nsga-net github\n",
    "        CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "        \n",
    "        train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                Cutout(cutout_length),  # from nsga-net github\n",
    "                transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "            ])\n",
    "\n",
    "        val_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "            ])\n",
    "\n",
    "        train_dataset = torchvision.datasets.CIFAR10(root=data_path, train=True,\n",
    "                                                download=True, transform=train_transform)\n",
    "\n",
    "        val_dataset = torchvision.datasets.CIFAR10(root=data_path, train=False,\n",
    "                                               download=True, transform=val_transform)\n",
    "        \n",
    "    else :\n",
    "        raise Exception(\"Data Error, Only CIFAR10 allowed for the moment\")\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args_train.batch_size,\n",
    "                                              shuffle=True, num_workers=args_train.workers)  \n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args_train.batch_size,\n",
    "                                             shuffle=False, num_workers=args_train.workers)    \n",
    "    \n",
    "    ###########################\n",
    "    # Train\n",
    "    ###########################\n",
    "    niters = len(train_loader)\n",
    "    niters = 1\n",
    "\n",
    "    lr_scheduler = LRScheduler(optimizer, niters, args_train)  # (default) args.step = [30, 60, 90], args.decay_factor = 0.1, args.power = 2.0\n",
    "    epoch_ = 0\n",
    "    \n",
    "    for epoch in range(start_epoch, args_train.epochs):\n",
    "        # train for one epoch\n",
    "        train(train_loader, NN_model, criterion, optimizer, lr_scheduler, epoch, args_train.print_freq, log_file_name)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, NN_model, criterion, epoch, log_file_name)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "#         is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        \n",
    "        epoch_ = epoch\n",
    "\n",
    "    return (-best_prec1, flops), epoch_  # Min (-val_accuracy, flops) 이므로 val_accuracy(top1)에 - 붙여서 return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_kyy.utils_hr import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 예시 생성\n",
    "grph_ex =make_random_graph_ex(15)\n",
    "\n",
    "## G matrix 구하기\n",
    "random.seed(1234)\n",
    "nds, inds,onds = get_graph_info(grph_ex)\n",
    "g_mat = get_g_matrix(nds)\n",
    "\n",
    "from utils_kyy.utils_hr import operation_dictionary\n",
    "operation_dict=operation_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encoding g_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## g_mat -> 한 차원으로 피는 것 (encoding individual 만드는 것)\n",
    "\n",
    "nsize =  g_mat.shape[0]\n",
    "g_encode = []\n",
    "\n",
    "for idx, col_idx in enumerate(range(1,nsize)):\n",
    "    row_idx = idx + 1\n",
    "    g_encode.extend(g_mat[:row_idx,col_idx].tolist())\n",
    "    \n",
    "## encoding 된 걸 다시 g_mat 복원\n",
    "\n",
    "decode_mat = np.ones((nsize,nsize)) \n",
    "# to initlize 7 (disconnected)\n",
    "decode_mat = decode_mat * 7\n",
    "\n",
    "\n",
    "row_idx = 0\n",
    "col_idx = 1\n",
    "for idx , v in enumerate(g_encode):\n",
    "    decode_mat[row_idx][col_idx] = v\n",
    "    \n",
    "    if row_idx + 1  == col_idx :\n",
    "        col_idx+= 1\n",
    "        row_idx = 0 \n",
    "    else :\n",
    "        row_idx += 1\n",
    "        \n",
    "        \n",
    "def gmat2ind(g_mat):\n",
    "    nsize =  g_mat.shape[0]\n",
    "    g_encode = []\n",
    "\n",
    "    for idx, col_idx in enumerate(range(1,nsize)):\n",
    "        row_idx = idx + 1\n",
    "        g_encode.extend(g_mat[:row_idx,col_idx].tolist())\n",
    "\n",
    "    return g_encode\n",
    "\n",
    "def ind2gmat(g_encode,nize):\n",
    "    decode_mat = np.ones((nsize,nsize)) \n",
    "    \n",
    "    # to initlize 7 (disconnected)\n",
    "    decode_mat = decode_mat * 7\n",
    "\n",
    "\n",
    "    row_idx = 0\n",
    "    col_idx = 1\n",
    "    for idx , v in enumerate(g_encode):\n",
    "        decode_mat[row_idx][col_idx] = v\n",
    "\n",
    "        if row_idx + 1  == col_idx :\n",
    "            col_idx+= 1\n",
    "            row_idx = 0 \n",
    "        else :\n",
    "            row_idx += 1\n",
    "            \n",
    "    return decode_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters \n",
    "args_train = EasyDict({\n",
    "    \n",
    "    'data_path' : 'D:/data/',\n",
    "    \n",
    "        \"data\": \"CIFAR10\",\n",
    "        \"num_classes\" : 10,\n",
    "        \n",
    "        \"graycode\" : False,\n",
    "        \"hr\" : True,\n",
    "        \n",
    "        \"input_dim\" : 3,\n",
    "        \"lr_mode\": \"cosine\",\n",
    "        \"warmup_mode\": \"linear\",    \n",
    "        \"base_lr\": 0.2,\n",
    "        \"momentum\": 0.9, \n",
    "        \"weight_decay\": 0.00005,\n",
    "        \"print_freq\": 100,\n",
    "        \n",
    "        \"epochs\": 2,\n",
    "        \"batch_size\": 80,\n",
    "        \"workers\": 0,\n",
    "        \n",
    "        \"warmup_epochs\": 0,\n",
    "        \"warmup_lr\": 0.0,\n",
    "        \"targetlr\": 0.0,\n",
    "    \n",
    "        \"nsize\" : 25\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "})\n",
    "\n",
    "data_path = args_train.data_path\n",
    "channels=109\n",
    "num_classes=10\n",
    "input_channel=3\n",
    "\n",
    "log_file_name = 'temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ind -> graph\n",
    "\n",
    "individual = gmat2ind(g_mat)\n",
    "\n",
    "graphs_list = [gmat2graph(g_mat) for i in range(3)]\n",
    "\n",
    "graphs = EasyDict({'stage_1': graphs_list[0],\n",
    "                   'stage_2': graphs_list[1],\n",
    "                   'stage_3': graphs_list[2]\n",
    "                  })\n",
    "\n",
    "\n",
    "\n",
    "# ind -> gmat\n",
    "\n",
    "gmats = [ind2gmat(individual,nsize) for i in range(3)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[7., 6., 7., 7., 0., 7., 7., 7., 7., 7., 7., 7., 7., 7., 1.],\n",
       "        [7., 7., 7., 3., 7., 0., 7., 7., 7., 7., 7., 7., 2., 7., 5.],\n",
       "        [7., 7., 7., 7., 7., 6., 7., 5., 7., 0., 7., 7., 7., 4., 7.],\n",
       "        [7., 7., 7., 7., 0., 4., 7., 7., 7., 7., 7., 7., 7., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 0., 7., 7., 7., 7., 7., 0., 7., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 5., 7., 6., 7., 7., 7., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 0., 7., 7., 6., 7., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 2., 7., 7., 7., 3., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 1., 0., 7., 5., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 0., 4., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 3., 1., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 0., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 0.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.]]),\n",
       " array([[7., 6., 7., 7., 0., 7., 7., 7., 7., 7., 7., 7., 7., 7., 1.],\n",
       "        [7., 7., 7., 3., 7., 0., 7., 7., 7., 7., 7., 7., 2., 7., 5.],\n",
       "        [7., 7., 7., 7., 7., 6., 7., 5., 7., 0., 7., 7., 7., 4., 7.],\n",
       "        [7., 7., 7., 7., 0., 4., 7., 7., 7., 7., 7., 7., 7., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 0., 7., 7., 7., 7., 7., 0., 7., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 5., 7., 6., 7., 7., 7., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 0., 7., 7., 6., 7., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 2., 7., 7., 7., 3., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 1., 0., 7., 5., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 0., 4., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 3., 1., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 0., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 0.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.]]),\n",
       " array([[7., 6., 7., 7., 0., 7., 7., 7., 7., 7., 7., 7., 7., 7., 1.],\n",
       "        [7., 7., 7., 3., 7., 0., 7., 7., 7., 7., 7., 7., 2., 7., 5.],\n",
       "        [7., 7., 7., 7., 7., 6., 7., 5., 7., 0., 7., 7., 7., 4., 7.],\n",
       "        [7., 7., 7., 7., 0., 4., 7., 7., 7., 7., 7., 7., 7., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 0., 7., 7., 7., 7., 7., 0., 7., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 5., 7., 6., 7., 7., 7., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 0., 7., 7., 6., 7., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 2., 7., 7., 7., 3., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 1., 0., 7., 5., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 0., 4., 7., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 3., 1., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 0., 7.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 0.],\n",
       "        [7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.]])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_kyy.utils_hr import operation_dictionary\n",
    "operation_dict=operation_dictionary()\n",
    "\n",
    "class double_unit(nn.Module):\n",
    "    def __init__(self, inplanes, outplanes, stride=1):\n",
    "        super(double_unit, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm2d(outplanes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(x)\n",
    "        #print(\"Double RELU output\",out.size())\n",
    "        out = self.bn(out)\n",
    "        #print(\"Double BN output\",out.size())\n",
    "        return out\n",
    "        \n",
    "class Triplet_unit(nn.Module):\n",
    "    def __init__(self, inplanes, outplanes, stride=1):\n",
    "        super(Triplet_unit, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv = depthwise_separable_conv_3x3(inplanes, outplanes, stride)\n",
    "        self.bn = nn.BatchNorm2d(outplanes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(x)\n",
    "        #print(\"Triplet Relu output\",out.size())\n",
    "        out = self.conv(out)\n",
    "        #print(\"Triplet Conv output\",out.size())\n",
    "        out = self.bn(out)\n",
    "        #print(\"Triplet BN output\",out.size())\n",
    "        return out\n",
    "\n",
    "class Node_OP(nn.Module):\n",
    "    def __init__(self, Node, inplanes, outplanes, gmat):\n",
    "        super(Node_OP, self).__init__()\n",
    "        self.is_input_node = Node.type == 0\n",
    "        self.input_nums = len(Node.inputs)  # 해당 Node에 input으로 연결된 노드의 개수\n",
    "        \n",
    "        ## For hierachical representation\n",
    "        self.id = Node.id\n",
    "        self.oplist = nn.ModuleList()\n",
    "        self.gmat = gmat\n",
    "        self.Node = Node\n",
    "        \n",
    "        #print(\"is it input {}, how many inputs {}\".format(self.is_input_node,self.input_nums))\n",
    "        # get operation from g_matirx\n",
    "        if self.input_nums >= 1:\n",
    "            for in_idx in Node.inputs:\n",
    "                operation_idx = gmat[in_idx][self.id]\n",
    "                op = operation_dict[operation_idx]\n",
    "                self.oplist.append(op(nin=outplanes, nout=outplanes, stride=1)) ## input node가 아닐 떄는 channel size 유지!!\n",
    "\n",
    "        # input 개수가 1보다 크면, 여러 input을 합쳐야함.\n",
    "        #print(\"input nums\",self.input_nums)\n",
    "        if self.input_nums >= 1:\n",
    "            self.mean_weight = nn.Parameter(torch.ones(self.input_nums))  # type: torch.nn.parameter.Parameter\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        if self.is_input_node:\n",
    "            self.conv = Triplet_unit(inplanes, outplanes, stride=2) \n",
    "        else:\n",
    "            self.conv = double_unit(outplanes, outplanes, stride=1)\n",
    "\n",
    "        \n",
    "    # [참고] nn.Sigmoid()(torch.ones(1)) = 0.7311\n",
    "    # seoungwonpark source 에서는 torch.zeros()로 들어감. => 0.5\n",
    "    def forward(self, *input):\n",
    "        #print(\"Node ID : {} input nums forward {} is it input node {}\".format(self.id,self.input_nums,self.is_input_node))\n",
    "        #print(\"Operation list\", self.oplist)\n",
    "        if self.input_nums > 1 and self.is_input_node == False:\n",
    "            #print('='*10)\n",
    "            #print(\"Node ID {} Components mean_weight : {} Operation type : {} input size : {}\".format(self.id, self.mean_weight[0].size(),\n",
    "                                                                                                    # self.oplist[0],input[0].size()))\n",
    "            out = self.sigmoid(self.mean_weight[0]) * self.oplist[0](input[0])  ## input node가 아니라고 생각해도 됨!\n",
    "            #print(\"Node ID : {} Output : {}\".format(self.id,out.size()))\n",
    "            for i in range(1, self.input_nums):\n",
    "                #print(\"input index {}, input : {} Operation Type {}\".format(i, input[i].size(),self.oplist[i]))\n",
    "                out = out + self.sigmoid(self.mean_weight[i]) * self.oplist[i](input[i])\n",
    "                #print(\"Input Node ID : {} Output : {}\".format(id,out.size()))\n",
    "\n",
    "            #print(\"Multi input Node output\",out)\n",
    "        elif self.input_nums == 1  and self.is_input_node == False:\n",
    "            out = self.sigmoid(self.mean_weight[0]) * self.oplist[0](input[0])  ## input node가 아니라고 생각해도 됨!\n",
    "            #print(\"Components \\n self.mean_weight : {} Operation : {} Sigmoid : {} input : {} \".format(self.mean_weight[0], self.oplist[0],self.sigmoid(self.mean_weight[0]),input[0].size()))\n",
    "\n",
    "            #print(\"node id : {} \\n after sigmoid Output : {}\".format(self.id,out.size()))\n",
    "\n",
    "        else :\n",
    "            out = input[0]\n",
    "            #print(\"Node ID : {} is a input Node \\n Output : {}\".format(self.id,out))\n",
    "\n",
    "        #print(\"node id : {}, before conv Out put : {}\".format(self.id,out))\n",
    "        out = self.conv(out)\n",
    "        \n",
    "        #print('='*50)\n",
    "        #print(\"node id : {}, Final Out put : {}\".format(self.id,out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class StageBlock(nn.Module):\n",
    "    def __init__(self, graph, inplanes, outplanes, gmat):\n",
    "        super(StageBlock, self).__init__()\n",
    "        # graph를 input으로 받아서, Node_OP class. 즉, neural network graph로 전환함.\n",
    "        self.nodes, self.input_nodes, self.output_nodes = get_graph_info(graph)\n",
    "        self.nodeop = nn.ModuleList()  # Holds submodules in a list.\n",
    "        self.gmat = gmat\n",
    "\n",
    "        for node in self.nodes:\n",
    "            # 각각의 node들을 Node_OP class로 만들어준 뒤, nn.ModuleList()인 self.nodeop에 append 해주기\n",
    "            self.nodeop.append(Node_OP(node, inplanes, outplanes, self.gmat))\n",
    "\n",
    "    def forward(self, x):\n",
    "        results = {}\n",
    "        # input\n",
    "        for id in self.input_nodes:\n",
    "            results[id] = self.nodeop[id](x)  # input x를 먼저 graph's input node에 각각 넣어줌.\n",
    "        # graph 중간 계산\n",
    "        for id, node in enumerate(self.nodes):\n",
    "            # 각각의 노드 id에 대해\n",
    "            if id not in self.input_nodes:\n",
    "                # graph's input node가 아니라면, 그래프 내에서 해당 노드의 인풋들인 node.inputs의 output인 results[_id]\n",
    "                #    => 그 결과를 results[id]에 저장.\n",
    "                # self.nodeop[id]는 해당 id의 Node_OP. 즉, input들을 받아서 forward(모아서, conv 태우기)하는 것.\n",
    "                # 따라서, input으로 넣을 때 unpack 함.\n",
    "                # id 작은 노드부터 result를 차근차근 계산하면서, id를 올라감.\n",
    "                #print(\"input size if id {}\".format(id))\n",
    "                results[id] = self.nodeop[id](*[results[_id] for _id in node.inputs])\n",
    "                \n",
    "                ## inspecting None\n",
    "                #print(self.nodeop[id](*[results[_id] for _id in node.inputs]))\n",
    "                #print(\"*** Starting id out : {}, ids in : {} , results : {}  node operation : {}\".format(id,node.inputs,results,self.nodeop[id]))\n",
    "                #print('='*30)\n",
    "                #print(\"Is Input node :  {} , Result id : {} Result Output shape : {} nodeop : {} \".format(id in self.input_nodes,\n",
    "                                                                                     #   id,results[id], self.nodeop[id]))\n",
    "\n",
    "        result = results[self.output_nodes[0]]\n",
    "        # output\n",
    "        # graph's output_nodes의 output 들을 평균내기\n",
    "        for idx, id in enumerate(self.output_nodes):\n",
    "            if idx > 0:\n",
    "                result = result + results[id]\n",
    "        result = result / len(self.output_nodes)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "class RWNN(nn.Module):\n",
    "    def __init__(self, net_type , graphs, gmats, channels, num_classes=1000, input_channel=3):\n",
    "        super(RWNN, self).__init__()\n",
    "\n",
    "        self.input_channel = input_channel\n",
    "        self.gmats = gmats\n",
    "        # 논문에서도 conv1 쪽은 예외적으로 Conv-BN 이라고 언급함. (나머지에서는 Conv-ReLU-BN 을 conv 로 표기)\n",
    "        #         self.conv1 = depthwise_separable_conv_3x3(input_channel, channels // 2, 2)    # nin, nout, stride\n",
    "        self.conv1 = depthwise_separable_conv_3x3(input_channel, channels // 2, 1)  # nin, nout, stride\n",
    "        self.bn1 = nn.BatchNorm2d(channels // 2)\n",
    "\n",
    "        # 채널수 변화도, 논문에서처럼 conv2: C, conv3: 2C, conv4: 4C, conv5: 8C\n",
    "        if net_type == 'small':\n",
    "            #             self.conv2 = Triplet_unit(channels // 2, channels, 2)    # inplanes, outplanes, stride=2\n",
    "            self.conv2 = Triplet_unit(channels // 2, channels, 1)  # inplanes, outplanes, stride=1\n",
    "\n",
    "            #             self.conv3 = StageBlock(graphs.stage_1, channels // 2, channels)\n",
    "            self.conv3 = StageBlock(graphs.stage_1, channels, channels,self.gmats[0])\n",
    "\n",
    "            self.conv4 = StageBlock(graphs.stage_2, channels, channels * 2, self.gmats[1])\n",
    "\n",
    "            self.conv5 = StageBlock(graphs.stage_3, channels * 2, channels * 4, self.gmats[2])\n",
    "\n",
    "            self.relu = nn.ReLU()\n",
    "\n",
    "            self.bn2 = nn.BatchNorm2d(channels * 4)\n",
    "            self.avgpool = nn.AvgPool2d(4, stride=1)  # 마지막은 global average pooling\n",
    "            self.fc = nn.Linear(channels * 4, num_classes)\n",
    "\n",
    "        #         if net_type == 'small':\n",
    "        #             self.conv2 = Triplet_unit(channels // 2, channels, 2)    # inplanes, outplanes, stride=2\n",
    "\n",
    "        #             self.conv3 = StageBlock(graphs.stage_1, channels, channels)\n",
    "\n",
    "        #             self.conv4 = StageBlock(graphs.stage_2, channels, channels *2)\n",
    "\n",
    "        #             self.conv5 = StageBlock(graphs.stage_3, channels * 2, channels * 4)\n",
    "\n",
    "        #             self.relu = nn.ReLU()\n",
    "        # #             self.conv = nn.Conv2d(channels * 4, 1280, kernel_size=1)   # 마지막에 1x1 conv, 1280-d\n",
    "        # #             self.bn2 = nn.BatchNorm2d(1280)\n",
    "\n",
    "        #######################################\n",
    "        # 원 코드에서 regular 부분 지움\n",
    "        #######################################\n",
    "        #         self.avgpool = nn.AvgPool2d(7, stride=1)  # 마지막은 global average pooling\n",
    "        #         self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"==========Data Size\", x.size())\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        #print(\"==========Conv1 Size\", x.size())  # => (-, 54, 16, 16)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        #print(\"==========Conv2  Size\", x.size())  # => (-, 109, 8, 8)\n",
    "        x = self.conv3(x)\n",
    "        #print(\"==========Conv3  Size\", x.size())  # => (-, 109, 8, 8)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        #         print(\"==========Conv4  Size\", x.size())  # => (-, 218, 4, 4)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        #print(\"==========Conv5  Size\", x.size())  # => (-, 436, 2, 2)\n",
    "\n",
    "        #         x = self.relu(x)  # => (-, 436, 1, 1)\n",
    "        #         x = self.conv(x)\n",
    "        #         print(\"==========Conv6  Size\", x.size())  # => (-, 1280, 1, 1)\n",
    "\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        #print(\"==========before avgpool  Size\", x.size())   ## [수정] CIFAR-10 에서는 여기까지오면 (-, 1280, 7, 7) 이 아니라 (-, ,1280, 1, 1)\n",
    "        x = self.avgpool(x)\n",
    "        #         print(\"==========after avgpool  Size\", x.size())\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_mat[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2) build RWNN\n",
    "channels = 109\n",
    "NN_model = RWNN(net_type='small', graphs=graphs, channels=109,gmats=gmats ,num_classes=10, input_channel=3)\n",
    "NN_model.cuda()\n",
    "\n",
    "###########################\n",
    "# Flops 계산 - [Debug] nn.DataParallele (for multi-gpu) 적용 전에 확인.\n",
    "###########################\n",
    "input_flops = torch.randn(1, input_channel, 32, 32).cuda()\n",
    "flops, params = profile(NN_model, inputs=(input_flops, ), verbose=False) ## 사이즈 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\t - Epoch: [0][0/625]\tTime 0.819 (0.819)\tLoss 2.3352 (2.3352)\tPrec@1 12.500 (12.500)\tPrec@5 58.750 (58.750)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-7801d6c8457d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;31m# train for one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNN_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# evaluate on validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\datascience\\git_repo\\RWNN-optimization\\utils_kyy\\train_validate.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, criterion, optimizer, lr_scheduler, epoch, print_freq, log_file_name)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;31m# measure elapsed time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\newrw\\lib\\site-packages\\torch\\optim\\sgd.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     99\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'momentum_buffer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                         \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Model summary\n",
    "#summary(NN_model, input_size=(1, 224, 224))\n",
    "\n",
    "# 3) Prepare for train### 일단 꺼보자!\n",
    "#NN_model = nn.DataParallel(NN_model)  # for multi-GPU\n",
    "#NN_model = nn.DataParallel(NN_model, device_ids=[0,1,2,3])\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(NN_model.parameters(), args_train.base_lr,\n",
    "                            momentum=args_train.momentum,\n",
    "                            weight_decay=args_train.weight_decay)\n",
    "\n",
    "start_epoch  = 0\n",
    "best_prec1 = 0    \n",
    "\n",
    "cudnn.benchmark = True    # This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware.  \n",
    "\n",
    "###########################\n",
    "# Dataset & Dataloader\n",
    "###########################\n",
    "\n",
    "# 이미 다운 받아놨으니 download=False\n",
    "# 데이터가 없을 경우, 처음에는 download=True 로 설정해놓고 실행해주어야함\n",
    "\n",
    "if data_path is None :\n",
    "    data_path = './data'\n",
    "\n",
    "\n",
    "if args_train.data == \"CIFAR10\" :\n",
    "\n",
    "    cutout_length = 16  # from nsga-net github\n",
    "\n",
    "    CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]  # from nsga-net github\n",
    "    CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "        ])\n",
    "\n",
    "    val_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "        ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=data_path, train=True,\n",
    "                                            download=True, transform=train_transform)\n",
    "\n",
    "    val_dataset = torchvision.datasets.CIFAR10(root=data_path, train=False,\n",
    "                                           download=True, transform=val_transform)\n",
    "\n",
    "else :\n",
    "    raise Exception(\"Data Error, Only CIFAR10 allowed for the moment\")\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args_train.batch_size,\n",
    "                                          shuffle=True, num_workers=args_train.workers)  \n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args_train.batch_size,\n",
    "                                         shuffle=False, num_workers=args_train.workers)    \n",
    "\n",
    "###########################\n",
    "# Train\n",
    "###########################\n",
    "niters = len(train_loader)\n",
    "niters = 1\n",
    "\n",
    "lr_scheduler = LRScheduler(optimizer, niters, args_train)  # (default) args.step = [30, 60, 90], args.decay_factor = 0.1, args.power = 2.0\n",
    "epoch_ = 0\n",
    "\n",
    "#for epoch in range(start_epoch, args_train.epochs):\n",
    "for epoch in range(start_epoch, 30):\n",
    "\n",
    "    # train for one epoch\n",
    "    train(train_loader, NN_model, criterion, optimizer, lr_scheduler, epoch, args_train.print_freq, log_file_name)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec1 = validate(val_loader, NN_model, criterion, epoch, log_file_name)\n",
    "\n",
    "    # remember best prec@1 and save checkpoint\n",
    "#         is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "\n",
    "    epoch_ = epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training 성공 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSGA 적용\n",
    "* Initialization  - WS으로 Graph 및 initialization 생성\n",
    "* Crossover - gray encodng 처럼 같은 stage 안에서만\n",
    "* Mutation - 요거는 생각해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import base, creator\n",
    "from deap import tools\n",
    "\n",
    "\n",
    "import random\n",
    "from itertools import repeat\n",
    "from collections import Sequence\n",
    "\n",
    "# For evaluate function --------------------------\n",
    "import glob\n",
    "from easydict import EasyDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn    # for hardware tunning (cudnn.benchmark = True)\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "import logging\n",
    "\n",
    "# Gray code package\n",
    "from utils_kyy.utils_graycode_v2 import *\n",
    "\n",
    "# custom package in utils_kyy\n",
    "from utils_kyy.utils_graph import load_graph\n",
    "from utils_kyy.models_hr import RWNN\n",
    "from utils_kyy.train_validate import train, validate, train_AMP\n",
    "from utils_kyy.lr_scheduler import LRScheduler\n",
    "from torchsummary import summary\n",
    "# -------------------------------------------------\n",
    "\n",
    "#from apex import amp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage_pool_path_list ~ graphs 만들어주는 과정 -> indivudal에서 바로 생성하는 걸로!\n",
    "\n",
    "\n",
    "def evaluate_hr_full_train(individual, args_train,data_path=args_train.data_path, channels=109, log_file_name=None):  # individual\n",
    "\n",
    "    graphs = []\n",
    "    gmats = []\n",
    "    \n",
    "    for ind in individual:\n",
    "        gmat = ind2gmat(ind, args_train.nsize)\n",
    "        gmats.append(gmat)\n",
    "        graphs.append(gmat2graph(gmat))\n",
    "\n",
    "    \n",
    "    graphs = EasyDict({'stage_1': graphs[0],\n",
    "                       'stage_2': graphs[1],\n",
    "                       'stage_3': graphs[2]\n",
    "                      })\n",
    "\n",
    "    # 2) build RWNN\n",
    "    channels = channels\n",
    "    NN_model = RWNN(net_type='small', graphs=graphs,gmats=gmats ,channels=channels, num_classes=args_train.num_classes, input_channel=args_train.input_dim)\n",
    "    NN_model.cuda()\n",
    "\n",
    "    ###########################\n",
    "    # Flops 계산 - [Debug] nn.DataParallele (for multi-gpu) 적용 전에 확인.\n",
    "    ###########################\n",
    "    input_flops = torch.randn(1, args_train.input_dim, 32, 32).cuda()\n",
    "    flops, params = profile(NN_model, inputs=(input_flops, ), verbose=False)\n",
    "\n",
    "    ## Model summary\n",
    "    #summary(NN_model, input_size=(1, 224, 224))\n",
    "\n",
    "    # 3) Prepare for train### 일단 꺼보자!\n",
    "    #NN_model = nn.DataParallel(NN_model)  # for multi-GPU\n",
    "    #NN_model = nn.DataParallel(NN_model, device_ids=[0,1,2,3])\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD(NN_model.parameters(), args_train.base_lr,\n",
    "                                momentum=args_train.momentum,\n",
    "                                weight_decay=args_train.weight_decay)\n",
    "    \n",
    "    start_epoch  = 0\n",
    "    best_prec1 = 0    \n",
    "    \n",
    "    cudnn.benchmark = True    # This flag allows you to enable the inbuilt cudnn auto-tuner to find the best algorithm to use for your hardware.  \n",
    "    \n",
    "    ###########################\n",
    "    # Dataset & Dataloader\n",
    "    ###########################\n",
    "\n",
    "    # 이미 다운 받아놨으니 download=False\n",
    "    # 데이터가 없을 경우, 처음에는 download=True 로 설정해놓고 실행해주어야함\n",
    "    \n",
    "    if data_path is None :\n",
    "        data_path = './data'\n",
    "    \n",
    " \n",
    "    if args_train.data == \"CIFAR10\" :\n",
    "\n",
    "        cutout_length = 16  # from nsga-net github\n",
    "        \n",
    "        CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]  # from nsga-net github\n",
    "        CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "        \n",
    "        train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "              #  Cutout(cutout_length),  # from nsga-net github\n",
    "                transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "            ])\n",
    "\n",
    "        val_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "            ])\n",
    "\n",
    "        train_dataset = torchvision.datasets.CIFAR10(root=data_path, train=True,\n",
    "                                                download=True, transform=train_transform)\n",
    "\n",
    "        val_dataset = torchvision.datasets.CIFAR10(root=data_path, train=False,\n",
    "                                               download=True, transform=val_transform)\n",
    "        \n",
    "    else :\n",
    "        raise Exception(\"Data Error, Only CIFAR10 allowed for the moment\")\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args_train.batch_size,\n",
    "                                              shuffle=True, num_workers=args_train.workers)  \n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args_train.batch_size,\n",
    "                                             shuffle=False, num_workers=args_train.workers)    \n",
    "    \n",
    "    ###########################\n",
    "    # Train\n",
    "    ###########################\n",
    "    niters = len(train_loader)\n",
    "    niters = 1\n",
    "\n",
    "    lr_scheduler = LRScheduler(optimizer, niters, args_train)  # (default) args.step = [30, 60, 90], args.decay_factor = 0.1, args.power = 2.0\n",
    "    epoch_ = 0\n",
    "    \n",
    "    for epoch in range(start_epoch, args_train.epochs):\n",
    "        # train for one epoch\n",
    "        train(train_loader, NN_model, criterion, optimizer, lr_scheduler, epoch, args_train.print_freq, log_file_name)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, NN_model, criterion, epoch, log_file_name)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "#         is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        \n",
    "        epoch_ = epoch\n",
    "\n",
    "    return (-best_prec1, flops), epoch_  # Min (-val_accuracy, flops) 이므로 val_accuracy(top1)에 - 붙여서 return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\t - Epoch: [0][0/625]\tTime 0.807 (0.807)\tLoss 2.3336 (2.3336)\tPrec@1 7.500 (7.500)\tPrec@5 46.250 (46.250)\n",
      "\t - Epoch: [0][100/625]\tTime 0.168 (0.186)\tLoss 1.7982 (2.1401)\tPrec@1 28.750 (22.611)\tPrec@5 86.250 (75.557)\n",
      "\t - Epoch: [0][200/625]\tTime 0.212 (0.183)\tLoss 1.7701 (1.9434)\tPrec@1 28.750 (28.619)\tPrec@5 86.250 (81.163)\n",
      "\t - Epoch: [0][300/625]\tTime 0.209 (0.183)\tLoss 1.5875 (1.8294)\tPrec@1 45.000 (32.728)\tPrec@5 86.250 (84.115)\n",
      "\t - Epoch: [0][400/625]\tTime 0.170 (0.182)\tLoss 1.3567 (1.7453)\tPrec@1 53.750 (35.842)\tPrec@5 91.250 (86.022)\n",
      "\t - Epoch: [0][500/625]\tTime 0.177 (0.181)\tLoss 1.2208 (1.6787)\tPrec@1 55.000 (38.363)\tPrec@5 98.750 (87.405)\n",
      "\t - Epoch: [0][600/625]\tTime 0.211 (0.182)\tLoss 1.1309 (1.6215)\tPrec@1 61.250 (40.566)\tPrec@5 97.500 (88.471)\n",
      "##### Validation_time 6.266 Prec@1 54.130 Prec@5 94.610 #####\n",
      "\t - Epoch: [1][0/625]\tTime 0.202 (0.202)\tLoss 1.2452 (1.2452)\tPrec@1 58.750 (58.750)\tPrec@5 95.000 (95.000)\n",
      "\t - Epoch: [1][100/625]\tTime 0.167 (0.179)\tLoss 1.4051 (1.2296)\tPrec@1 52.500 (55.532)\tPrec@5 95.000 (94.616)\n",
      "\t - Epoch: [1][200/625]\tTime 0.209 (0.179)\tLoss 1.1438 (1.2106)\tPrec@1 62.500 (56.393)\tPrec@5 92.500 (94.813)\n",
      "\t - Epoch: [1][300/625]\tTime 0.165 (0.180)\tLoss 1.2350 (1.1739)\tPrec@1 57.500 (57.924)\tPrec@5 95.000 (95.361)\n",
      "\t - Epoch: [1][400/625]\tTime 0.172 (0.179)\tLoss 0.8568 (1.1431)\tPrec@1 71.250 (59.074)\tPrec@5 97.500 (95.745)\n",
      "\t - Epoch: [1][500/625]\tTime 0.170 (0.180)\tLoss 0.9066 (1.1143)\tPrec@1 65.000 (60.157)\tPrec@5 96.250 (95.936)\n",
      "\t - Epoch: [1][600/625]\tTime 0.166 (0.180)\tLoss 1.0700 (1.0921)\tPrec@1 57.500 (61.054)\tPrec@5 93.750 (96.050)\n",
      "##### Validation_time 6.242 Prec@1 65.660 Prec@5 97.020 #####\n"
     ]
    }
   ],
   "source": [
    "# sample individuals to train\n",
    "\n",
    "nds , _, _  = get_graph_info(grph_ex)\n",
    "individuals = [gmat2ind(get_g_matrix(nds)) for i in range(3)]\n",
    "fitness_value , epoch = evaluate_hr_full_train(individuals,args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-65.66, 983578624.0) 1\n"
     ]
    }
   ],
   "source": [
    "## Check evaluation funtion\n",
    "print(fitness_value, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For initializing random Graph WS\n",
    "\n",
    "def init_graph(nsize):\n",
    "    # nsize : number of nodes\n",
    "    \n",
    "    individuals = []\n",
    "    \n",
    "    for i in range(3) : # For s stages\n",
    "        grph_ex =make_random_graph_ex(nsize) # Make random graph using WS\n",
    "\n",
    "        ## G matrix 구하기\n",
    "        nds, inds,onds = get_graph_info(grph_ex)\n",
    "        g_mat = get_g_matrix(nds)\n",
    "\n",
    "        individual = gmat2ind(g_mat)\n",
    "        individuals.extend(individual)\n",
    "    \n",
    "    return individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tool box for hr\n",
    "# individual 바뀌어야\n",
    "\n",
    "def create_toolbox_for_NSGA_RWNN_hr(args_train, data_path=None ,log_file_name=None):\n",
    "    # => Min ( -val_accuracy(top_1),  flops )\n",
    "    creator.create('FitnessMin', base.Fitness, weights=(-1.0, -1.0 ))  # name, base (class), attribute // \n",
    "    creator.create('Individual', list, fitness=creator.FitnessMin)  # creator.FitnessMaxMin attribute로 가짐    \n",
    "    \n",
    "    #####################################\n",
    "    # Initialize the toolbox\n",
    "    #####################################\n",
    "    toolbox = base.Toolbox()\n",
    "    if args_train.graycode :\n",
    "        gray_len = len(str(grayCode(num_graph-1)))\n",
    "        IND_SIZE = gray_len * 3\n",
    "        BOUND_LOW = 0\n",
    "        BOUND_UP = 1\n",
    "        toolbox.register('attr_int', random.randint, BOUND_LOW, BOUND_UP)\n",
    "    \n",
    "    \n",
    "    elif args_train.hr :\n",
    "        ## Nsize -> individual size\n",
    "        nsize = args_train.nsize\n",
    "        BOUND_LOW = 0 \n",
    "        BOUND_UP = 7 # Operation 종류\n",
    "        #toolbox.register('attr_int', random.randint, BOUND_LOW, BOUND_UP) \n",
    "        \n",
    "        toolbox.register('init_graph',nsize)\n",
    "        \n",
    "        ## WS 으로 초기화시키는 함수를 만들자, \n",
    "        # input : nsize\n",
    "        # output : nsize -> Graph -> Gmat -> indivdual로 연계되게\n",
    "        \n",
    "        \n",
    "\n",
    "    else:\n",
    "        IND_SIZE = 3    # 한 individual, 즉 하나의 chromosome은 3개의 graph. 즉, 3개의 stage를 가짐.\n",
    "\n",
    "        # toolbox.attribute(0, (num_graph-1)) 이렇게 사용함.\n",
    "        # 즉, 0 ~ (num_grpah - 1) 중 임의의 정수 선택. => 이걸 3번하면 하나의 small graph가 생김\n",
    "        BOUND_LOW = 0\n",
    "        BOUND_UP = num_graph-1\n",
    "        toolbox.register('attr_int', random.randint, BOUND_LOW, BOUND_UP)   # register(alias, method, argument ...)\n",
    "        # toolbox.attribute라는 함수를 n번 시행해서 containter인 creator.individual에 넣은 후 해당 instance를 반환함.\n",
    "        # e.g. [0, 1, 3] 반환\n",
    "        \n",
    "    ## HR 일 때는 WS로 initiat 하고 싶은데 이 부분 좀 생각이 필요할듯\n",
    "    toolbox.register('individual', tools.initRepeat,\n",
    "                     creator.Individual, toolbox.init_graph, nsize)\n",
    "\n",
    "    toolbox.register('population', tools.initRepeat,\n",
    "                     list, toolbox.individual)    # n은 생략함. toolbox.population 함수를 뒤에서 실행할 때 넣어줌.    \n",
    "    \n",
    "    # crossover\n",
    "    if args_train.graycode :\n",
    "        toolbox.register('mate', cxgray, num_graph=num_graph)\n",
    "    \n",
    "    elif args_train.hr :\n",
    "        toolbox.register('mate',cxhr)\n",
    "    else:\n",
    "        toolbox.register('mate', tools.cxTwoPoint)  # crossover\n",
    "\n",
    "    # mutation\n",
    "    toolbox.register('mutate', mutUniformInt_custom, low=BOUND_LOW, up=BOUND_UP)\n",
    "\n",
    "    # selection\n",
    "    # => return A list of selected individuals.\n",
    "    toolbox.register('select', tools.selNSGA2, nd='standard')  # selection.  // k – The number of individuals to select. k는 함수 쓸 때 받아야함\n",
    "    \n",
    "    #########################\n",
    "    # Seeding a population - train_log 읽어와서 해당 log의 마지막 population으로 init 후 이어서 train 시작\n",
    "    #########################\n",
    "    # [Reference] https://deap.readthedocs.io/en/master/tutorials/basic/part1.html\n",
    "    def LoadIndividual(icls, content):\n",
    "        return icls(content)\n",
    "\n",
    "    def LoadPopulation(pcls, ind_init, last_population):  # list of [chromosome, [-val_accuracy, flops]]\n",
    "        return pcls(ind_init(last_population[i][0]) for i in range(len(last_population)))\n",
    "\n",
    "    toolbox.register(\"individual_load\", LoadIndividual, creator.Individual)\n",
    "\n",
    "    toolbox.register(\"population_load\", LoadPopulation, list, toolbox.individual_load)\n",
    "    \n",
    "    return toolbox\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GA function test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_graph(nsize):\n",
    "    # nsize : number of nodes\n",
    "\n",
    "    individuals = []\n",
    "\n",
    "    for i in range(3):  # For s stages\n",
    "        grph_ex = make_random_graph_ex(nsize)  # Make random graph using WS\n",
    "\n",
    "        ## G matrix 구하기\n",
    "        nds, inds, onds = get_graph_info(grph_ex)\n",
    "        g_mat = get_g_matrix(nds)\n",
    "\n",
    "        individual = gmat2ind(g_mat)\n",
    "        individuals.extend(individual)\n",
    "\n",
    "    return individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from easydict import EasyDict\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "from deap import tools\n",
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "import json\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from utils_kyy.utils_hr import make_random_graph_ex\n",
    "from utils_kyy.create_toolbox_hr import create_toolbox_for_NSGA_RWNN_hr, evaluate_hr_full_train\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "class rwns_train:\n",
    "    def __init__(self, json_file):\n",
    "        self.root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "        self.param_dir = os.path.join(self.root + '/parameters/', json_file)\n",
    "        f = open(self.param_dir)\n",
    "        params = json.load(f)\n",
    "        pprint(params)\n",
    "        self.name = params['NAME']\n",
    "\n",
    "        ## toolbox params\n",
    "        self.args_train = EasyDict(params['ARGS_TRAIN'])\n",
    "        self.data_path = params['DATA_PATH']\n",
    "        self.run_code = params['RUN_CODE']\n",
    "        self.stage_pool_path = '../graph_pool' + '/' + self.run_code + '_' + self.name + '/'\n",
    "        self.stage_pool_path_list = []\n",
    "        for i in range(1, 4):\n",
    "            stage_pool_path_i = self.stage_pool_path + str(i) + '/'  # eg. [graph_pool/run_code_name/1/, ... ]\n",
    "            self.stage_pool_path_list.append(stage_pool_path_i)\n",
    "        \n",
    "        self.log_path = '../logs/' + self.run_code + '_' + self.name + '/'\n",
    "        # self.log_file_name : Initialize 부터 GA 진행상황 등 코드 전체에 대한 logging\n",
    "        self.log_file_name = self.log_path + 'logging.log'\n",
    "        # self.train_log_file_name : fitness (= flops, val_accuracy). 즉 GA history 를 저장 후, 나중에 사용하기 위한 logging.\n",
    "        self.train_log_file_name = self.log_path + 'train_logging.log'\n",
    "        \n",
    "        if not os.path.exists(self.stage_pool_path):\n",
    "            os.makedirs(self.stage_pool_path)\n",
    "            for i in range(3):\n",
    "                os.makedirs(self.stage_pool_path_list[i])\n",
    "                \n",
    "        if not os.path.isdir(self.log_path):\n",
    "            os.makedirs(self.log_path)\n",
    "            \n",
    "        logging.basicConfig(filename=self.log_file_name, level=logging.INFO)\n",
    "        logging.info('[Start] Rwns_train class is initialized.')        \n",
    "        logging.info('Start to write log.')\n",
    "            \n",
    "        self.num_graph = params['NUM_GRAPH']\n",
    "        \n",
    "        self.toolbox = self.create_toolbox()\n",
    "\n",
    "        \n",
    "        ## GA params\n",
    "        self.pop_size = params['POP_SIZE']\n",
    "        self.ngen = params['NGEN']\n",
    "        self.cxpb = params['CXPB']\n",
    "        self.mutpb = params['MUTPB']\n",
    "\n",
    "        ## logs\n",
    "        self.log = OrderedDict()\n",
    "        self.log['hp'] = self.args_train\n",
    "        self.train_log = OrderedDict()\n",
    "        \n",
    "\n",
    "    def create_toolbox(self):\n",
    "        make_random_graph_ex(self.nsize)\n",
    "\n",
    "        return create_toolbox_for_NSGA_RWNN(self.num_graph, self.args_train, self.data_path, self.log_file_name)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        ###################################\n",
    "        # 1. Initialize the population.  (toolbox.population은 creator.Individual n개를 담은 list를 반환. (=> population)\n",
    "        ###################################\n",
    "        now = datetime.datetime.now()\n",
    "        now_str = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(\"[GA] Initialion starts ...\")\n",
    "        logging.info(\"[GA] Initialion starts at \" + now_str)\n",
    "        init_start_time = time.time()\n",
    "\n",
    "        pop = self.toolbox.population(n=self.pop_size)\n",
    "\n",
    "        # fitness values (= accuracy, flops) 모음\n",
    "        GA_history_list = []    # (choromosome, accuracy, flops) 이렇게 담아놓기\n",
    "                                # e.g.  [ [[1,3,4], 40, 1000], [[2,6,10], 30%, 2000], ...  ]\n",
    "        \n",
    "        ###################################\n",
    "        # 2. Evaluate the population (with an invalid fitness)\n",
    "        ###################################\n",
    "        invalid_ind = [ind for ind in pop]\n",
    "\n",
    "        for idx, ind in enumerate(invalid_ind):\n",
    "            fitness, ind_model = evaluate_v2(ind, args_train=self.args_train, stage_pool_path_list=self.stage_pool_path_list,\n",
    "                                          data_path=self.data_path, log_file_name=self.log_file_name)\n",
    "            ind.fitness.values = fitness\n",
    "            GA_history_list.append([ind, fitness])\n",
    "            \n",
    "        ## log 기록 - initialize (= 0th generation)\n",
    "        self.train_log[0] = GA_history_list\n",
    "        \n",
    "        self.save_log()\n",
    "        \n",
    "        # This is just to assign the crowding distance to the individuals\n",
    "        # no actual selection is done\n",
    "        pop = self.toolbox.select(pop, len(pop))\n",
    "\n",
    "        now = datetime.datetime.now()\n",
    "        now_str = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(\"Initialization is finished at\", now_str)\n",
    "        logging.info(\"Initialion is finished at \" + now_str)\n",
    "\n",
    "        init_time = time.time() - init_start_time\n",
    "        logging.info(\"Initialization time = \" + str(init_time) + \"s\")\n",
    "        print()\n",
    "        \n",
    "        ###################################\n",
    "        # 3. Begin GA\n",
    "        ###################################\n",
    "        # Begin the generational process\n",
    "        for gen in range(1, self.ngen):\n",
    "            ##### 3.1. log 기록\n",
    "            now = datetime.datetime.now()\n",
    "            now_str = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"#####\", gen, \"th generation starts at\", now_str)\n",
    "            logging.info(\"#####\" + str(gen) + \"th generation starts at\" + now_str)\n",
    "\n",
    "            start_gen = time.time()\n",
    "            \n",
    "            ##### 3.2. Offspring pool 생성 후, crossover(=mate) & mutation\n",
    "            # Vary the population\n",
    "            offspring = tools.selTournamentDCD(pop, len(pop))\n",
    "            offspring = [self.toolbox.clone(ind) for ind in offspring]\n",
    "            \n",
    "            # ::2, 1::2 즉, 짝수번째 크로모좀과 홀수번쨰 크로모좀들 차례로 선택하면서 cx, mut 적용\n",
    "            # e.g. 0번, 1번 ind를 cx, mut   // 2번, 3번 ind를 cx, mut // ...\n",
    "            for ind1, ind2 in zip(offspring[::2], offspring[1::2]):\n",
    "                if random.random() <= self.cxpb:\n",
    "                    self.toolbox.mate(ind1, ind2)\n",
    "\n",
    "                self.toolbox.mutate(ind1, indpb=self.mutpb)\n",
    "                self.toolbox.mutate(ind2, indpb=self.mutpb)\n",
    "                del ind1.fitness.values, ind2.fitness.values\n",
    "\n",
    "            ##### 3.3. Evaluation\n",
    "            # Evaluate the individuals with an invalid fitness\n",
    "            print(\"\\t Evaluation...\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            # fitness values (= accuracy, flops) 모음\n",
    "            GA_history_list = []\n",
    "            \n",
    "            invalid_ind = [ind for ind in offspring]\n",
    "            \n",
    "            for idx, ind in enumerate(invalid_ind):\n",
    "                fitness, ind_model = evaluate_v2(ind, args_train=self.args_train, stage_pool_path_list=self.stage_pool_path_list, data_path=self.data_path, log_file_name=self.log_file_name)\n",
    "                # <= evaluate() returns  (-prec, flops), NN_model\n",
    "                \n",
    "                ind.fitness.values = fitness\n",
    "                GA_history_list.append([ind, fitness])\n",
    "\n",
    "            ## log 기록\n",
    "            self.train_log[gen] = GA_history_list\n",
    "            \n",
    "            self.save_log()\n",
    "\n",
    "            eval_time_for_one_generation = time.time() - start_time\n",
    "            print(\"\\t Evaluation ends (Time : %.3f)\" % eval_time_for_one_generation)\n",
    "\n",
    "            ##### Select the next generation population\n",
    "            pop = self.toolbox.select(pop + offspring, self.pop_size)\n",
    "\n",
    "            gen_time = time.time() - start_gen\n",
    "            print('\\t [gen_time: %.3fs]' % gen_time, gen, 'th generation is finished.')\n",
    "\n",
    "            logging.info('\\t Gen [%03d/%03d] -- evals: %03d, evals_time: %.4fs, gen_time: %.4fs' % (\n",
    "                gen, self.ngen, len(invalid_ind), eval_time_for_one_generation, gen_time))\n",
    "\n",
    "\n",
    "    ## Save Log\n",
    "    def save_log(self):\n",
    "        ## 필요한 log 추후 정리하여 추가 \n",
    "        self.log['train_log'] = self.train_log\n",
    "\n",
    "        with open(self.train_log_file_name, 'w', encoding='utf-8') as make_file:\n",
    "            json.dump(self.log, make_file, ensure_ascii=False, indent='\\t')\n",
    "\n",
    "            \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--params', type=str, help='Parameter Json file')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    trainer = rwns_train(json_file=args.params)\n",
    "#     trainer.create_toolbox()\n",
    "    trainer.train()\n",
    "    trainer.save_log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'newrw'",
   "language": "python",
   "name": "newrw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
